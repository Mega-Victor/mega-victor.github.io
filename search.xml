<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java POI解析 Excel]]></title>
    <url>%2F2017%2F12%2F16%2FJava%20POI%20%E8%A7%A3%E6%9E%90Excel%2F</url>
    <content type="text"><![CDATA[坑1POI解析excel的时候, 取每个cell的值的时候需要先使用 cell.getCellType(), 去判断该cell的数据的类型(比如是 string还是boolean还是空之类的),这时候如果是日期类型,就会遇到”坑1”,日期类型是会被判断成_Cell.CELL_TYPENUMERIC ,如果想读取数据为指定时间格式,可以使用以下代码: 1234567SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyy-MM-dd&quot;); //日期格式化switch (cell.getCellType()) &#123; .... .... case Cell.CELL_TYPE_NUMERIC: if (DateUtil.isCellDateFormatted(cell)) &#123; value = sdf.format(cell.getDateCellValue()); 上面这个格式化数据代码还同时填了”坑1”自带的小坑, 算是个隐藏的坑, 那就是Excel中默认日期格式是没有”yyyy-mm-dd”这个格式的, 也就是如果Excel中的数据需要设置”yyyy-mm-dd”这样格式是需要自定义格式的, 如果你自定义了这种日期格式, 那么在POI读取的时候就会解析成奇怪的数据格式, 形如”16日-12月-2017年”等. 上面这个代码段也解决的这个小坑. 无论本来的Excel中数据格式是”yyyy/mm/dd” 还是”yyyy-mm-dd” 等都可以完美的解析成功. 坑2POI解析 0.00 和 0 这两种自然数数据, 我们会要求吧小数解析成”0.00”格式 而 整数解析成 0 不包含小数点的格式. 由于 小数和整数 都是cell.getCellType()==Cell.CELL_TYPE_NUMERIC, 所以需要一个方法一次性处理成功两种数据类型, 改要小数点后面的就保留, 不该要的(整数) 就不要加上烦人的整数.00这种. 代码如下: 1234567891011121314SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyy-MM-dd&quot;); //日期格式化switch (cell.getCellType()) &#123; .... .... case Cell.CELL_TYPE_NUMERIC: if (DateUtil.isCellDateFormatted(cell)) &#123; value = sdf.format(cell.getDateCellValue()); &#125; else &#123; Double d = cell.getNumericCellValue(); DecimalFormat decimalFormat = new DecimalFormat(&quot;#.##&quot;); String numValue = decimalFormat.format(d); value = numValue; &#125; break; ps: 这种情况下有时候如果Entity使用Float的话会把小数解析成很长的一串, 比如本来数据是3.26, 他会解析成3.26000000323之类的,虽然换算成两位小数精度不变, 但是还是会影响存取显示. 所以你需要使用BigDecimal 而不是Float. 坑3这个坑填了好久, 主要是太奇葩了. 描述一下坑怎么来的:比如上传的Excel有10行数据,当你解析Excel的时候全部解析成功, 但是由于某种原因你不要第十行了, 那你直接选定第十行然后Del了 ,然后你再解析, 会发现还会读取第十行, 虽然你已在逐行读取的时候过滤设置了” row==null” 这个条件, 但就是会读取该行. 慢慢调节发现虽然删除了数据, 但是数据格式还保留在第10行, 逐行读取的时候还是会读取该行的数据格式作为一行,虽然没有数据. 除非你直接删除该行, 而不是简单使用常用的del. 由于用户还是会使用del 的,所以就要判断del数据但是还保存着数据格式的行. 代码如下: 123456789for (int j = 1; j &lt;= sheet.getLastRowNum(); j++) &#123; row = sheet.getRow(j); if (row == null) &#123; continue; &#125; if (new ImportExcelUtil().getCellValue(row.getCell(0)).toString().equals(&quot;&quot;) &amp; new ImportExcelUtil().getCellValue(row.getCell(1)).toString().equals(&quot;&quot;)) &#123; continue; &#125;]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>POI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven dubbo-2.8.4导入教程]]></title>
    <url>%2F2017%2F12%2F07%2Fmaven%20dubbo-2.8.4%E5%AF%BC%E5%85%A5%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[maven中心库没有dubbo2.8.4,需要自己编译。我已经编译好了jar包 安装dubbo-2.8.4.jar包到maven仓库中如下操作： 1mvn install:install-file -Dfile=/Users/username/Downloads/dubbo-2.8.4.jar -DgroupId=com.alibaba -DartifactId=dubbo -Dversion=2.8.4 -Dpackaging=jar -DgeneratePom=true 需要dubbo-2.8.4.jar的联系我的邮箱就可以, 相信你会找到我的邮箱的]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala 上传本地数据]]></title>
    <url>%2F2017%2F11%2F30%2FImpala%20%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1. 使用location 方式这种方式下在impala的HDFS的db目录下直接指定一个和表名一样的目录就可以, 注意此目录当前在db目录下是不存在的. 也就是创建表的时候才会创建相应的目录. 说的不太清晰,请看代码就明白了 12345create external table device(group_id string ,device_id string)row format delimited fields terminated by '\t'lines terminated by '\n'location "hdfs://impala/warehouse/portal.db/device/" 注意这里建表是使用的外部表, external, EXTERNAL_TABLE 这样建的表被drop之后HDFS上面的文件依然还是存在的, 否则就是内部表, 删除表的同时HDFS上的文件一起删除了 注意这个/device/本来是不存在的, 创建表的时候才会创建这个目录后面需要加载到表的数据直接上传到这个目录下面就行, 然后再refresh +table name 一下,更新一下数据, 代码如下 123hadoop fs -put id.txt hdfs://impala/warehouse/portal.db/devicerefresh device # 在 impala shell 中进行 2.使用load方式这种方式是可以导入放在任意HDFS路径下的文件, 但是当文件被load的时候是采用的move方式, 也就是剪贴. 1234567# 建表create external table device1(group_id string ,device_id string)row format delimited fields terminated by '\t'lines terminated by '\n'# 加载数据load data inpath "hdfs://impala/tmp/id.txt" into (overwrite) table device 增加分区也可以使用这种方式, 假设我们按照group_id进行分区, 123456789101112# 建表create table device(device_id string)partitioned by (group_id string)row format delimited fields terminated by '\t'lines terminated by '\n'# 加载数据到新的分区# 增加分区数据(这里使用alter 是因为不存在当前分区,是新加的分区)alter table device add partition(dgroup_id="1") location "hdfs://...../id.txt"# 继续load数据到当前分区load data inpath "hdfs://hz-impala1/tmp/id.txt" into table device partition(group_id=1)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 下提示/boot空间不足解决办法]]></title>
    <url>%2F2017%2F11%2F29%2FUbuntu%20%3Aboot%20%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%2F</url>
    <content type="text"><![CDATA[查看当前内核1uname -a 查看内核12dpkg --get-selections|grep linuxlinux-后面带image的是旧的内核 清除不使用的内核123sudo apt-get autoremove linux-image-(版本号)或者sudo apt-get remove linux-image-(版本号)（就是上面带image的版本） 清理残留数据1dpkg -l |grep ^rc|awk '&#123;print $2&#125;' |sudo xargs dpkg -P]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase 上传CSV文件]]></title>
    <url>%2F2017%2F11%2F29%2F%E6%8A%8Acsv%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%88%B0hdfs%20%2F</url>
    <content type="text"><![CDATA[把csv文件上传到hdfs12hadoop fs -mkdir /datahadoop fs -put file.csv /data hbase shell 新建表1create 'table','col1','col2' ####使用importtsv命令上传 1./hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,col1，col2 '-Dimporttsv.separator=,' table /data/file.csv 以上命令要把csv文件的第一列作为row，剩下的作为列族。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive UDF 开发]]></title>
    <url>%2F2017%2F11%2F29%2FHive-UDF-%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[何为UDF?这个大家肯定已经都知道了,就是user defined function,当Hive内置函数不能满足你的业务需求的时候,就可以自己写一个UDF进行处理 需求需要把Hive表中含map格式的数据导入到impala中,在impala中使用map格式数据. 但是由于impala中取map格式数据的方式比较奇葩,无法进行map中的多个key进行group by. 同时为了方便拉取数据,所以暂时需要把hive中map转成json string 拉到impala中,然后再对json string进行解析. UDF编写过程1.自定义一个Java类 2.继承UDF类 3.重写evaluate方法 4.打成jar包 5.在hive执行add jar方法 6.在hive执行创建模板函数 7.hql中使用 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.datacube.hive.udf;import java.util.ArrayList;import java.util.Collections;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDF;@Description(name = "to_string", value = "_FUNC_(map) - Change map to String ")public class MapToString extends UDF&#123; public String evaluate(Map&lt;String, String&gt; a) &#123; if (a == null) &#123; return null; &#125; try &#123; ArrayList&lt;String&gt; r = new ArrayList&lt;String&gt;(a.size()); for (Map.Entry&lt;String, String&gt; entry : a.entrySet()) &#123; r.add("\""+entry.getKey()+"\"" + ":" + "\""+entry.getValue()+"\"" + ","); &#125; //Collections.sort(r); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; r.size(); i++) &#123; sb.append(r.get(i)); &#125; sb.deleteCharAt(sb.length() - 1); sb.insert(0,"&#123;"); sb.insert(sb.length(),"&#125;"); return sb.toString(); &#125;catch (Exception e)&#123; return null; &#125; &#125; public static void main(String[] args) &#123; Map&lt;String,String&gt; map =new HashMap&lt;String, String&gt;(); map.put("name","xiaoming"); map.put("age","18"); System.out.println(new MapToString().evaluate(map)); &#125; &#125; 下面是json string 的解析 写了两种方式,但是效率都不太高 123456789101112131415161718192021222324252627282930313233343536package com.datacube.hive.udf;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;@Description(name = "map_filter", value = "_FUNC_(string) - Get the Value of the Key ")public class MapFilter extends UDF &#123; public String evaluate(Text s, String target) &#123; if (s == null || target == null) &#123; return null; &#125; String[] kvPairs = s.toString().replace("\"", "").replace("&#125;","").replace("&#123;","").split(","); try &#123; String targetValue = null; for (String kvPair : kvPairs) &#123; String[] kv = kvPair.split(":"); String key = kv[0]; String value = kv[1]; if (key.equals(target)) &#123; targetValue=value; &#125; &#125; return targetValue; &#125; catch (Exception e) &#123; return null; &#125; &#125; public static void main(String[] args) &#123; String aa = "&#123;\"rid\":\"1511321168403\",\"column\":\"头条\",\"RCC\":\"D3RFE69O0001875P\",\"type\":\"doc\",\"offset\":\"2\"&#125;"; System.out.println(new MapFilter().evaluate(new Text(aa),"column")); &#125;&#125; 另一种解析方式是用的Gson 1234567891011121314151617public class MapFilterUDF extends UDF&#123; public String evaluate(Text s, String targetKey) &#123; if (s == null || targetKey == null) &#123; return null; &#125; try &#123; Gson gson = new Gson(); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map = gson.fromJson(s.toString(), map.getClass()); return map.get(targetKey); &#125; catch (Exception e)&#123; return null; &#125; &#125; 使用mvn package 打包,在Hive shell 中使用以下命令 123add jar "/...jar"CREATE TEMPORARY FUNCTION to_string AS '包名.MapToString(类名)';CREATE TEMPORARY FUNCTION map_filter AS '包名.MapFilter(类名)'; 可以使用description function +自定义函数名 查看描述 在UDF里面使用@Description已经把描述加进去了,方便使用者进行查看函数的描述]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摇滚客 | 蠢货王尼玛其人]]></title>
    <url>%2F2017%2F11%2F27%2F%E6%91%87%E6%BB%9A%E5%AE%A2%20%7C%20%E8%A0%A2%E8%B4%A7%E7%8E%8B%E5%B0%BC%E7%8E%9B%E5%85%B6%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[原文 原创来自微信公众号摇滚客,见文章最后二维码 距离《暴走大事件》全网下架过去整整十天了。 10天，240个小时，恍若隔世。 一个叫王尼玛的大头死变态，摘下了头套，活像一个“蠢货”。 作为一个知名脱口秀主持人，作为一个一千六百万粉丝的微博大V，竟然想不开去干愤青的事。 一面势单力薄地挑战不可能战胜的巨兽，一面发了疯似的煽动年轻人和他一起反抗，你说他蠢不蠢？ 豫章书院说没有学生自杀那就没人自杀，人家这么正规一学校能骗你吗？ 人家家长为了孩子好，跑去声援学校，你非骂人家“帮凶”，不封你封谁呢？ 人人都说皇帝的新衣五光十色，好看得不得了，你非毁谤人家没穿衣服。幼稚！可笑！ 活该节目被下架整改，回归遥遥无期，你骂学校、骂家长就算了，还固执地叫人家孩子动脑子，学会反抗。 喋喋不休，活像居委会大妈，人家又不是你儿子，你干嘛这么关心，一副恨铁不成钢的样子。 这不，这档烂摊子没解决，他又不怕死地跳出来，就红黄蓝幼儿园的事发声。 你自己说说你是不是蠢货？ （一） 80、90年代，曾经是文艺作品喷薄而出的春天。 那个年代的电影、电视剧创作没有这么多条条框框，更加注重个性。 特别是拍给孩子看的儿童剧、动画片多颜色荒诞，配乐诡异、剧情邪典，有着严重的反乌托邦色彩。 《魔方大厦》、《镜花缘》、《红气球》、《十二生肖》、《眉间尺》、《黑猫警长》，甚至是《葫芦娃》… 随便挑出一部，都能让看过的观众，记起一两个童年阴影的片段，至今记忆犹新。 很少人知道，这些苦涩的镜头，包含着那代艺术创作前辈们的良苦用心。 早年看黑猫警长，觉得动画片惩恶扬善，看犯罪份子被击毙有一种儿时才有的快感。 但—— 随着年龄渐涨，再看这部片子就发现了异端。 黑猫警长的世界实际一个法律一点都不健全的社会啊！ 警察局要靠秘密监视、窃听才能控制整个森林的稳定，时时消灭异己，才能保证和谐。 他们是怎么面对伏法的罪犯的？ 白鸽警员将罪犯的眼睛遮住，五花大绑，用皮带打得皮开肉绽，一旁的警长却无动于衷，眼神冰冷。 相信很多人还记得螳螂夫妇那集，对杀死蝗虫群的镜头，不断地进行特写： 螳螂夫妇挥刀斩杀了大批飞来的蝗虫，有的被腰斩、有的被肢解，还有的直接断了头。 接着警长到达现场，一把火烧死了这些蝗虫，并且大笑着说： “哈哈哈，这是最好的肥料啦！” 满地尸体、残肢乱飞的场面，很容易造成小观众的不适。 至于导演为什么这么拍，有人说，极有可能是创作人员在宣泄对83年严打的恐惧： 那个时代，组织家庭舞会会因为邻居的举报坐牢，偷窃少量财物会被判死刑。 当年有个叫当红的歌手叫做迟志强，就因为跳舞的舞伴坐在腿上，坐了四年牢，理由是聚众淫乱。 和他一起参加舞会的年轻人，无一幸免，一个小伙因为偷看女厕，判了死刑，缓期两年。 很多现代的年轻人对次并不了解，只有那些触目惊醒的往事还在述说那段讳莫如深的历史： 四川一个小伙和朋友打赌，亲了路边一陌生女孩，结果直接被枪毙，一命呜呼； 某青年因为喝多了在路边小解，被抓了现行，二话不说扭送新疆劳改； 还有一个男青年，看见街上的洋妞长得丰满，一时冲动摸了一把，也被处以枪决； … 拍完第五集《黑猫警长》就被紧急叫停了，至于原因导演有这样一段描述： 那天我被叫去人事处，他们递给我一张退休证，说我年龄到了，该退了。醒过神来后，我一句话没说，拿了退休证转身就走。 现在你还觉得《黑猫警长》仅仅是一部惩恶扬善的动画片吗？ 如果是的话你可以自己观察黑猫警长的武器——鲁格枪，纳粹盖世太保的标配，集权、恐怖的代名词。 第五集请看下集几个大字之后，便再也没有了消息… （二） 其实，给更多人留下童年阴影的片子是《魔方大厦》。 如果让滚君用两个字来形容这个片子，只有两个字—— 癫狂。 死人脸、怪诞的场景、诡异的剧情，不管是导演还是原作，都在尽力用隐晦的语言向我们表达些什么。 第二集，来克被一群小将们选为市长，上台后他将家长被关进了罐头，并贴上大字报，无情批判。 孩子们从此无拘无束，尽情玩耍打砸抢烧、让民生社会陷入了一片黑暗之中，但越是这样，市民们就约支持来克。 也许，创作者就是希望用这种方式，告诉那代孩子，无论什么时候都不要丧失理智，千万不要重蹈历史的覆辙。 长相怪异、尸白吊眼的干部 无法无天打砸抢烧的小将 还有消失在历史深处的大字报 还有，在个国家，人民不允许出现负面情绪，必须活在正能量中。 那些脸上笑嘻嘻，心理mmp的人，都会被集中起来，用特殊设备抽干净你的情绪。 从此，你就变成一个不哭不闹的乖孩子了。 除了一个叫怪里怪气的人，他不服为什么别人说好，他就要说好，为什么别人笑，他就要笑—— 他是一个独立的人啊！ 于是他想尽各种方法和这个世界作对，只不过，这一切都是徒劳，他最终战胜了自己，他爱老大哥。 在魔方大厦里，作者笑话失去个性的人终生都无法摆脱“禁锢的头脑，可笑的规矩”，还直言“不能说真话不如当哑巴！” 他还对“卫生部门的门外汉领导，非要插手文艺作品的审核”，进行了无情嘲弄，嬉笑怒骂快意恩仇。 诸如此类的情节还很多，这也就是为什么这部片子在播了十集之后，就遭到腰斩。 剩下的十六集遥遥无期，来克也被永远地关在了那座魔方里，出不来了。 有的时候，我们心疼来克再不能回家，可是看看我们，岂不是跟他一样，也迷失在一个现实版的“魔方大厦”里了。 在魔方大厦里，作者笑话失去个性的人终生都无法摆脱“禁锢的头脑，可笑的规矩”，还直言“不能说真话不如当哑巴！” 他还对“卫生部门的门外汉领导，非要插手文艺作品的审核”，进行了无情嘲弄，嬉笑怒骂快意恩仇。 诸如此类的情节还很多，这也就是为什么这部片子在播了十集之后，就遭到腰斩。 剩下的十六集遥遥无期，来克也被永远地关在了那座魔方里，出不来了。 有的时候，我们心疼来克再不能回家，可是看看我们，岂不是跟他一样，也迷失在一个现实版的“魔方大厦”里了。 （三） 岂止是动画片，80、90后接触到的电视剧、电影，很多都被那个时代的艺术工作者赋予了隐喻。 比如那部号称情景喜剧的鼻祖《我爱我家》，第一集就用春秋笔法映射了反复无常的政治运动。 还有那部《疯狂的兔子》，将人群在失去判断力之后的癫狂，庸众的可怕表现得淋漓尽致。 唯有一直清醒，坚定自我的人，才能走到最后… 公众号X博士说：那个年代的所有大师，都在暗暗教你如何成为一个独立而不屈的人。 创作者想尽各种办法把将经历过的痛苦、留下的伤疤通过另一种方式，传递给下一代。 即使他们明明知道这样的表达会造成下一代的反感，给他们留下童年阴影。 但，他们心里清楚，这就像打防疫针，了解了社会的真相，才能更勇敢地走下去—— 动动脑子，你们得学会反抗，反抗啊，少年… （四） 生于1990年的王尼玛就是在这种环境下成长起来的。 不管他的教育经历是怎样的，曾经那些前辈们创作出来的东西，都在他的脑中起到了潜移默化的作用。 也许很多80、90后，比起00后，有一股子劲儿，大概就是这个原因吧。 我们的童年里没有低幼的喜羊羊、熊出没，而是在阴影下生存，在不屈中呐喊的技巧啊！ 后来王尼玛长大了，办了自己的脱口秀。 虽然他可能还没意识到，但他确实在重复着曾经那些前辈们做过的事。 他一直在用不同的观点鞭笞社会给年轻人的偏见和不公啊！ 他带上头条，想让这些尖锐的观点不那么敏感，但他还是刺痛了一些人。 他说，即使是跪着，前方的路，我们也会一起走完。 所以，在节目的最后他说了这样一件事： 一个读者想考研，但父母逼他考公务员，他现在很苦难不知道怎么办。 王尼玛没有开玩笑，而是忧心忡忡的说，语气里充满了恨铁不成钢的感觉： “你要考研，谁拦得住，自己打工赚钱，自己考研。养了五年的宠物，不想送人谁拦得住，自己租房自己养它，谁拦得住？” 因为充满热血的人，至死都是少年！ “动动脑子，动动脑子，少年，要反抗，反抗嘛。动动脑子嘛，动动脑子，反抗，动动脑子！” 这和二十年前那些前辈们所说的，有什么不同呢？ 只不过那些前辈，用更聪明、更隐晦的方式。而王尼玛，就这样傻傻地送了人头。 除了暴走大事件，几乎所有同类节目都面临着寒冬期。 凤凰卫视官方发布消息，节目走过了19个年头后迎来停播。 关于节目为何停播的消息众说纷纭，当在微博上搜索关键字时，只有明晃晃的一排字： “根据相关法律法规和政策，搜索结果未予显示。 但其实这几乎是一个必然。有网友发现，一切早有预兆。 根据镜像娱乐调查显示：“在《锵锵》之前，“老梁”系列、《鲁豫有约》、《超级访问》、《壹周立波秀》、《今晚80后脱口秀》、《金星秀》等都没能逃过停播的命运。” 语言类、谈话类节目一一阵亡，有的销声匿迹，有的则转战网络。 但是网络环境又真的有想象中的那么好吗？ 6月份，“新浪微博”、“ACFUN”、“凤凰网”的视听节目服务被关，一时之间地动山摇。 为了生存下来，“新浪微博”严禁上传长视频，A站濒临崩溃、B站要求实名。无数网络节目都改名的改名、下架的下架。 诚如网友所说：该来的总会来的，一切不和谐的综艺都会被毙掉，不分电视、网络，没有意外！ 终于，今天，王尼玛也闭上了嘴。 想到之前有人用这么两句话来形容李银河： 多么愚蠢，非要闹到不能张嘴，非要闹到自己出点事。这对许多中国人来说，难道不是愚蠢的吗？ 然而，吾辈永远要记得，他捍卫的不是他一个人的自由，而是千千万万你我他的自由。所做的就是在为了你们呐喊啊！ 有人说王尼玛愤青、有人说他自寻死路，但是他所做的，不都是在告诉年轻人，你们要思考，你们学会反抗啊！ 就算举身赴死，也要义不容辞！ 永远，永远也不要因为习惯了黑暗，就去为黑暗辩护。 永远要记得，为众人抱薪者，不可使其冻毙于风雪！ 暴走大事件没了，王尼玛还是坚持在微博上表明自己对红黄蓝事件的态度，他是蠢，还蠢的那么可爱。 套用一句老话来说： 尽管我们脑子里的反叛改变不了这个世界，但它却是我们真正拥有的东西，是我们最后一寸领土。 在那一寸领土里，我们是自由的！]]></content>
      <categories>
        <category>千里之堤,做只蚂蚁</category>
      </categories>
      <tags>
        <tag>和谐社会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 文件数量，大小统计]]></title>
    <url>%2F2017%2F11%2F27%2FHDFS-%E6%96%87%E4%BB%B6%E6%95%B0%E9%87%8F%EF%BC%8C%E5%A4%A7%E5%B0%8F%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[####统计HDFS文件数量大小，小于20M文件数量 hadoop fs -du -h / # 统计文件大小 hadoop fs -count / # 统计文件数量，返回的数据是目录个数，文件个数，文件总计大小，输入路径 hadoop fs -ls -R /path/data | grep ^- &gt; ~/data.txt #统计所有文件的信息，过滤文件夹,只统计文件.因为使用ls -l 之后,可以看到文件是”-“开头文件夹是”d”开头 再写个小python 123456import pandas as pdpath='/Desktop/data.txt'df=pd.read_table(path,delim_whitespace=True,names=[1,2,3,4,5,6,7,8]) # 统计数据一共8列print(len(df))df1=df[df[5]&lt;20971520] # 第五列是大小,取小于20M(换算成b)的文件数据print(len(df1)) # 统计数量]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala Complex Types]]></title>
    <url>%2F2017%2F11%2F26%2FImpala-Complex-Types%2F</url>
    <content type="text"><![CDATA[impala不支持直接insert complex type data,需要从hive中或者从parquet file中取出来.而且需要使用parquet格式的表.官方文档如下: Because the Impala INSERT statement does not currently support creating new data with complex type columns, or copying existing complex type values from one table to another, you primarily use Impala to query Parquet tables with complex types where the data was inserted through Hive, or create tables with complex types where you already have existing Parquet data files. If you have created a Hive table with the Parquet file format and containing complex types, use the same table for Impala queries with no changes. If you have such a Hive table in some other format, use a Hive CREATE TABLE AS SELECT … STORED AS PARQUET or INSERT … SELECT statement to produce an equivalent Parquet table that Impala can query. If you have existing Parquet data files containing complex types, located outside of any Impala or Hive table, such as data files created by Spark jobs, you can use an Impala CREATE TABLE … STORED AS PARQUET statement, followed by an Impala LOAD DATA statement to move the data files into the table. As an alternative, you can use an Impala CREATE EXTERNAL TABLE statement to create a table pointing to the HDFS directory that already contains the data files. 具体demo步骤如下: ####创建hive表 12345create table array_map_1(id string , column_name array&lt;string&gt;,info map&lt;string, string&gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'; ####导入数据到hive中 1LOAD DATA local INPATH '/home/weblog/demodata' INTO TABLE array_map_1 ; 1234567# 数据如下1|体育,娱乐,科技,社会|name:tom,age:16,city:beijing2|娱乐,社会,NBA,时尚,历史|name:jerry,age:173|汽车,NBA,健康|name:alice,city:shanghai4|视频,政务|name:harry,age:16,city:jinan5|体育,军事|name:danny,age:17,city:6|体育,政务,军事,话题,历史,社会|name:liming,city:dalian 因为hive中的parquet表map,array不支持直接导入txt之类的非parquet格式数据,我也暂时不知道怎么创建parquet格式文件,只能暂时创建普通表格导入数据,再select到一个parquet表中. ####创建新的parquet表格并导入数据 12create table array_map(id string,column_name array&lt;string&gt;,info map&lt;string, string&gt;) stored as parquet;insert into array_map select id,column_name,info from array_map_1; ####查看array_map的hdfs路径 12345describe formatted array_map;...| Location: | hdfs://localhost:20500/test-warehouse/tpch_nested_parquet.db/array_map ... ####查看hdfs具体路径 1234$ hdfs dfs -ls hdfs://localhost:20500/test-warehouse/tpch_nested_parquet.db/array_mapFound 4 items-rwxr-xr-x 3 dev supergroup 171298918 2015-09-22 23:30 hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/000000_0... ####在impala shell中把数据拉过去 123CREATE TABLE array_map LIKE PARQUET 'hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/000000_0' STORED AS PARQUET location 'hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/'; 最终结果显示]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git notes]]></title>
    <url>%2F2017%2F11%2F24%2Fgit-notes%2F</url>
    <content type="text"><![CDATA[其他命令* git init 在当前路径下初始化创建一个repository * git config --global alias.st status 把status配置成st --global参数是全局参数， 也就是这些命令在这台电脑的所有Git仓库下都有用。配置文件在C:\Users\username\.gitconfig 工作区和暂存区- git diff 是工作区(work dict)和暂存区(stage)的比较 - git diff --cached 是暂存区(stage)和分支(master)的比较 ###撤销修改 * 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 * 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令 git reset HEAD file，就回到了场景1，第二步按场景1操作。 ###添加文件到Git仓库 - git add +文件名 注意，可反复多次使用，添加多个文件； - git commit -m +&quot;描述&quot;（每次修改，如果不add到暂存区，那就不会加入到commit中） - git status 命令可以让我们时刻掌握仓库当前的状态 - git diff +文件名 查看文件做出过哪些修改（需要在git add之前查看） ###回退 - git log 查看历史记录 git log --pretty=oneline 把每个记录显示成一行便于查看 - git reset --hard HEAD^ HEAD表示当前版本,HEAD^表示上一个，HEAD~100表示向上100个 （WIN10 下使用git reset --hard HEAD~） - git reset --hard 66e315f （commit id） - git reflog 查看历史输入记录，进而查到commit id 回退后想再撤回，只需要找到想撤回到的版本使用的commit命令生成的commit id就可以，比如 git reset –hard 66e315f 但是如果关了电脑之前的命令看不到的话，可以使用git reflog查看历史输入记录。Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL。 ###删除文件 - git rm + 文件名 删除版本库中的文件。 - git checkout -- +文件名 把版本中的文件恢复到工作区 ###添加远程库 - git remote add origin git@server-name:path/repo-name.git 要关联一个远程库， - git push -u origin master 关联后第一次推送master分支的所有内容； - git push origin master 此后，每次本地提交后，只要有必要，就可以使用命令 git push origin master推送最新修改； ###创建删除分支 - 查看分支： git branch - 创建分支 ： git branch +分支名 - 切换分支 ： git checkout +分支名 - 创建+切换分支： git checkout -b+分支名 在本地创建和远程分支 对应的分支， 使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； - 合并某分支到当前分支 ： git merge +分支名 - 删除分支： git branch -d +分支名 - 强行删除分支 git branch -D +分支名 - git merge --no-ff -m &quot;描述&quot; +分支名删除分支后，不会丢掉分支信息，仍然在远程库中保留。 合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 ###合并冲突Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。git log –graph –pretty=oneline –abbrev-commit 看到分支的合并情况。。。 ###Bug分支当前分支只完成一半不能提交，但是需要马上去另一个分支修复bug，需要使用 - git stash 当前工作现场“储藏”起来，等以后恢复现场后继续工作 - git stash list 查看储存的工作现场 - git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； - git stash pop，恢复的同时把stash内容也删了： ###多人协作步骤 - 首先，可以试图用git push origin branch-name推送自己的修改； - 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； - 如果合并有冲突，则解决冲突，并在本地提交； - 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream branch-name origin/branch-name。 ###标签 - git tag &lt;name&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id； - git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息； - git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;可以用PGP签名标签； - git tag可以查看所有标签。 - git push origin &lt;tagname&gt;可以推送一个本地标签； - git push origin --tags可以推送全部未推送过的本地标签； - git tag -d &lt;tagname&gt;可以删除一个本地标签； - git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。 ###忽略特殊文件创建 .gitignore文件，并push到github 忽略文件的原则是： 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 git add -f App.class如果文件被忽略可以强制添加 git check-ignore命令检查规则 例如： $ git check-ignore -v App.class .gitignore:3:*.class App.class ###远程主机 * git remote 查看远程库信息 git remote -v 查看更详细的信息 * git remote show &lt;主机名&gt; 可以查看该主机的详细信息 * git remote add &lt;主机名&gt; &lt;网址&gt; 用于添加远程主机 * git remote rm &lt;主机名&gt; 删除远程主机 * git remote rename &lt;原主机名&gt; &lt;新主机名&gt; 此笔记在学习廖雪峰Git教程进行的记录。见http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000]]></content>
      <categories>
        <category>通用技术</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
</search>
