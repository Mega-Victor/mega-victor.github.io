<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[阿里云/腾讯云搭建hadoop namenode未启动]]></title>
    <url>%2F2018%2F05%2F14%2F%E9%98%BF%E9%87%8C%E4%BA%91%3A%E8%85%BE%E8%AE%AF%E4%BA%91%E6%90%AD%E5%BB%BAhadoop%20namenode%E6%9C%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[腾讯云 阿里云 分别或者混合搭建hadoop 端口占用先排除掉，然后还有错误： FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.java.net.BindException: Problem binding to [master:9000] java.net.BindException: Cannot assign requested address; For more details see: http://wiki.apache.org/hadoop/BindExceptionat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 一般出现这个问题是因为网络配置的原因。云主机有自己的内网ip，和外网ip。在配置hosts时，我配置了 123ip1 masterip2 slaver1ip3 slaver3 这里需要特别注意，这个ip你到底应该写什么。一开始我配置的时候在每台服务器中的hosts第一条都加了 127.0.0.1 hostname（每台机器的本机名），这样启动的时候jps命令下各个服务都起来了。但是这样master：9000就绑定在了127.0.0.1：9000上，直接浏览器就无法访问各个ui服务界面了。我发现后就把127.0.0.1 hostname这一条删除了。然后再重启，就一直出现上面的“Problem binding to [master:9000]”错误。把namenode都format好几遍也没用。废了很大力气才发现根本原因是内网 外网ip的问题。 问题是：如果是在master上操作的话ip必须是master 的内网ip 同理slaves上也是一样，自己的主机上的IP要改成内网IP，其他的要用外网IP。比如说在master上，hosts要这么写： 123内网ip master外网ip slaver1外网ip slaver2 关于内网外网ip 内网ip，内网也就是局域网，内网的计算机以NAT（网络地址转换）协议，通过一个公共的网关访问Internet。内网的计算机可向Internet上的其他计算机发送连接请求，但Internet上其他的计算机无法向内网的计算机发送连接请求。不可以直接用于服务器远程登录，其主要作用是：跟当前帐号下的其他同集群的机器通信。一些小型企业或者学校，通常都是申请一个固定的IP地址，然后通过IP共享（IP Sharing），使用整个公司或学校的机器都能够访问互联网。而这些企业或学校的机器使用的IP地址就是内网IP，内网IP是在规划IPv4协议时，考虑到IP地址资源可能不足，就专门为内部网设计私有IP地址（或称之为保留地址），一般常用内网IP地址都是这种形式的：10.X.X.X、 172.16.X.X-172.31.X.X、192.168.X.X等。需要注意的是，内网的计算机可向Internet上的其他计算机发送连接请求，但Internet上其他的计算机无法向内网的计算机发送连接请求。 服务器公网ip ，可以用于域名解析ip，服务器远程登录ip，是最主要的服务器ip地址。 公网IP就是除了保留IP地址以外的IP地址，可以与Internet上的其他计算机随意互相访问。我们通常所说的IP地址，其实就是指的公网 IP。互联网上的每台计算机都有一个独立的IP地址，该IP地址唯一确定互联网上的一台计算机。这里的IP地址就是指的公网IP地址。 其实，互联网上的计算机是通过“公网IP＋内网IP”来唯一确定的，就像很多大楼都是201房间一样，房间号可能一样，但是大楼肯定是唯一的。公网IP地址和内网IP地址也是同样，不同企业或学校的机器可能有相同的内网IP地址，但是他们的公网IP地址肯定不同。那么这些企业或学校的计算机是怎样IP地址共享的呢？这就需要使用NAT（Network Address Translation,网络地址转换）功能。当内部计算机要连接互联网时，首先需要通过NAT技术，将内部计算机数据包中有关IP地址的设置都设成NAT主机的公共IP地址，然后再传送到Internet，虽然内部计算机使用的是私有IP地址，但在连接Internet时，就可以通过NAT主机的 NAT技术，将内网我IP地址修改为公网IP地址，如此一来，内网计算机就可以向Internet请求数据了。 此处master的IP对于slaves是公网IP，但是对于master自己应该用内网IP—正真的对应自己的机器的ifconfig。 以上在master，salve1和slave2上都要执行，为了方便，jdk的安装目录最好一样，之后的目录操作也是。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[服务器免密登录设置]]></title>
    <url>%2F2018%2F05%2F14%2F%E6%9C%8D%E5%8A%A1%E5%99%A8A%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95B%2F</url>
    <content type="text"><![CDATA[服务器免密登录设置 服务器A免密登录B A 中生成 ssh-keygen 生成公钥和私钥 ：idrsa idrsa.pub。 确保生成的 idrsa 文件权限是600 idrsa.pub 文件权限是622,否则会在B在验证A是否具有登录B的权限的时候会出错，拒绝登录。 在B中的.ssh下面的authorizedkeys文件中增加A的idrsa.pub 中的key，确保B的 .ssh 文件权限是700，authorized_keys 文件权限是600 更改/etc/ssh/sshd_config 这一步在新的服务器中通常需要做 1234567vi /etc/ssh/sshd_configRSAAuthentication yes # 启用 RSA 认证PubkeyAuthentication yes # 启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同） 设置完之后记得重启SSH服务，才能使刚才设置有效。 service sshd restart]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[腾讯云主机hosts自动还原]]></title>
    <url>%2F2018%2F05%2F14%2F%E8%85%BE%E8%AE%AF%E4%BA%91%E4%B8%BB%E6%9C%BAhosts%E8%87%AA%E5%8A%A8%E8%BF%98%E5%8E%9F%2F</url>
    <content type="text"><![CDATA[腾讯云主机更改hosts之后，重启机器后hosts自动还原问题，这个问题在阿里云的云主机中不会出现，更改时云主机的机制不同。 这问题折腾了一两个小时，主要原因还是自己不够严谨，在更改hosts文件的时候没有好好读给的注释，hosts中的注释内容如下： Your system has configured ‘manage_etc_hosts’ as True. As a result, if you wish for changes to this file to persist then you will need to either a.) make changes to the master file in /etc/cloud/templates/hosts.redhat.tmpl b.) change or remove the value of ‘manage_etc_hosts’ in /etc/cloud/cloud.cfg or cloud-config from user-data 那个 ‘manage_etc_hosts’ 配置没找到，通过更改hosts.redhat.tmpl解决了，这个文件中的内容就是每次自动还原，覆盖你的/etc/hosts 的模板内容，只要把 /etc/cloud/templates/hosts.redhat.tmpl 这个文件内容更改hosts的同时 更改成你想要的内容就行。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JMH基准测试]]></title>
    <url>%2F2018%2F04%2F09%2FJMH%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[正如 Kirk Pepperdine 说过的：Measure, don’t guess. 代码性能测试的时候，应该建立一个稳定的性能测试环境（包括操作系统，jvm，应用服务器，数据库等），设置一些性能目标，针对这一目标不断的进行测试，直到达到你的预期。和持续测试、持续交付类似，我们也应该进行持续的性能测试。 我们使用JMH进行实例测试。 算是有两种方式吧 方式一 建立单独的JMH工程测试 建立maven工程 123$ mvn archetype:generate -DinteractiveMode=false -DarchetypeGroupId=org.openjdk.jmh \ -DarchetypeArtifactId=jmh-java-benchmark-archetype -DarchetypeVersion=1.4.1 \ -DgroupId=org.agoncal.sample.jmh -DartifactId=test -Dversion=1.0 进入工程后 会看到自动生成的这个代码 12345678910import org.openjdk.jmh.annotations.Benchmark;public class MyBenchmark &#123; @Benchmark public void testMethod() &#123; // place your benchmarked code here &#125;&#125; 在该方法下写代码即可,或者直接运行都能看到效果 写完后使用maven命令打包就能生成一个benchmarks.jar 12mvn clean installjava -jar target/benchmarks.jar 就能看到类似的结果了 123456789101112# Fork: 1 of 10# Warmup: 20 iterations, 1 s each# Measurement: 20 iterations, 1 s each# Threads: 1 thread, will synchronize iterations# Benchmark mode: Throughput, ops/time# Benchmark: org.sample.MyBenchmark.testMethod# Warmup Iteration 1: 3144380.099 ops/ms# Warmup Iteration 2: 3133745.492 ops/ms# Warmup Iteration 3: 3112635.234 ops/ms# Warmup Iteration 4: 3061073.932 ops/ms# Warmup Iteration 5: 3068514.482 ops/ms# Warmup Iteration 6: 3118373.441 ops/ms 方式二 使用idea-jmh-plugin直接插件中搜索jmh，首先安装好插件重启。然后在任意一个需要测试的java文件下右键，选择generate..,选择生成jmh，就会自动生成一个如下方法： 123@Benchmark public void measureName() &#123; &#125; maven 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-core&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.HashMap;import java.util.Map;import com.google.gson.Gson;import org.openjdk.jmh.annotations.Benchmark;import org.openjdk.jmh.annotations.Fork;import org.openjdk.jmh.annotations.Measurement;import org.openjdk.jmh.annotations.Warmup;/** * @author Mega-Victor * @date 2018/4/09 */@OutputTimeUnit(TimeUnit.MILLISECONDS)@Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)@Fork(1)public class TestBenchmark &#123; @Benchmark public String measureName() &#123; String s = &quot;&#123;\&quot;duflag\&quot;:\&quot;start\&quot;,\&quot;column\&quot;:\&quot;冷启动\&quot;,\&quot;dtype\&quot;:\&quot;tab\&quot;,\&quot;duid\&quot;:\&quot;1508322526\&quot;,\&quot;name\&quot;:\&quot;tom\&quot;,\&quot;age\&quot;:\&quot;16\&quot;,\&quot;city\&quot;:\&quot;beijing&#125;&quot;; String targetKey = &quot;duflag&quot;; if (s == null || targetKey == null) &#123; return null; &#125; try &#123; Gson gson = new Gson(); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map = gson.fromJson(s.toString(), map.getClass()); return map.get(targetKey); &#125; catch (Exception e) &#123; return null; &#125; &#125; @Benchmark public String measureName1() &#123; String s = &quot;&#123;\&quot;duflag\&quot;:\&quot;start\&quot;,\&quot;column\&quot;:\&quot;冷启动\&quot;,\&quot;dtype\&quot;:\&quot;tab\&quot;,\&quot;duid\&quot;:\&quot;1508322526\&quot;,\&quot;name\&quot;:\&quot;tom\&quot;,\&quot;age\&quot;:\&quot;16\&quot;,\&quot;city\&quot;:\&quot;beijing&#125;&quot;; String targetKey = &quot;duflag&quot;; if (s == null || targetKey == null) &#123; return null; &#125; String[] kvPairs = s.toString().replace(&quot;\&quot;&quot;, &quot;&quot;).replace(&quot;&#125;&quot;,&quot;&quot;).replace(&quot;&#123;&quot;,&quot;&quot;).split(&quot;,&quot;); try &#123; String targetValue = null; for (String kvPair : kvPairs) &#123; String[] kv = kvPair.split(&quot;:&quot;); String key = kv[0]; String value = kv[1]; if (key.equals(targetKey)) &#123; targetValue=value; &#125; &#125; return targetValue; &#125; catch (Exception e) &#123; return null; &#125; &#125; 和Junit测试一样直接运行代码，然后会看到结果，进行比较就会发现代码之间的性能差异 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# JMH version: 1.20# VM version: JDK 1.8.0_144, VM 25.144-b01# VM invoker: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre/bin/java# VM options: -Dfile.encoding=UTF-8# Warmup: 5 iterations, 1 s each# Measurement: 5 iterations, 1 s each# Timeout: 10 min per iteration# Threads: 1 thread, will synchronize iterations# Benchmark mode: Throughput, ops/time# Benchmark: DataStructures.StringDemo.measureName# Run progress: 0.00% complete, ETA 00:00:20# Fork: 1 of 1# Warmup Iteration 1: 62.469 ops/ms# Warmup Iteration 2: 98.835 ops/ms# Warmup Iteration 3: 117.189 ops/ms# Warmup Iteration 4: 118.060 ops/ms# Warmup Iteration 5: 117.986 ops/msIteration 1: 116.963 ops/msIteration 2: 117.225 ops/msIteration 3: 118.285 ops/msIteration 4: 119.339 ops/msIteration 5: 125.611 ops/msResult &quot;DataStructures.StringDemo.measureName&quot;: 119.485 ±(99.9%) 13.675 ops/ms [Average] (min, avg, max) = (116.963, 119.485, 125.611), stdev = 3.551 CI (99.9%): [105.809, 133.160] (assumes normal distribution)# JMH version: 1.20# VM version: JDK 1.8.0_144, VM 25.144-b01# VM invoker: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre/bin/java# VM options: -Dfile.encoding=UTF-8# Warmup: 5 iterations, 1 s each# Measurement: 5 iterations, 1 s each# Timeout: 10 min per iteration# Threads: 1 thread, will synchronize iterations# Benchmark mode: Throughput, ops/time# Benchmark: DataStructures.StringDemo.measureName1# Run progress: 50.00% complete, ETA 00:00:10# Fork: 1 of 1# Warmup Iteration 1: 191.825 ops/ms# Warmup Iteration 2: 218.092 ops/ms# Warmup Iteration 3: 251.509 ops/ms# Warmup Iteration 4: 258.351 ops/ms# Warmup Iteration 5: 267.683 ops/msIteration 1: 268.787 ops/msIteration 2: 273.119 ops/msIteration 3: 267.273 ops/msIteration 4: 268.032 ops/msIteration 5: 266.316 ops/msResult &quot;DataStructures.StringDemo.measureName1&quot;: 268.706 ±(99.9%) 10.133 ops/ms [Average] (min, avg, max) = (266.316, 268.706, 273.119), stdev = 2.632 CI (99.9%): [258.572, 278.839] (assumes normal distribution)# Run complete. Total time: 00:00:20**Benchmark Mode Cnt Score Error UnitsDataStructures.StringDemo.measureName thrpt 5 119.485 ± 13.675 ops/msDataStructures.StringDemo.measureName1 thrpt 5 268.706 ± 10.133 ops/ms**Process finished with exit code 0 可以看到以上代码使用Gson和直接手动解析json成map数据两种实现方式，手动解析速度相比更快一点]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle Hive Impala的总结比较]]></title>
    <url>%2F2018%2F01%2F09%2Foracle%20Hive%20Impala%20%E6%80%BB%E7%BB%93%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[&nbsp; &nbsp; &nbsp;转载自http://blog.csdn.net/mayp1/article/details/51415833 感谢 damipingzi 总结的这么详细&nbsp; &nbsp; &nbsp;分为五部分介绍，以表格形式对比。各软件默认版本如下：Oracle 11g, Hive 0.13.0, Impala 2.1.0，如有例外会指出。空白说明没有这种类型或功能。&nbsp; &nbsp; &nbsp;Cloudera官方文档有一篇文章阐述了Impala和Hive的差异（http://www.cloudera.com/documentation/archive/impala/2-x/2-1-x/topics/impala_langref_unsupported.html），不过描述得比较简略，详细的对照本文会给出。Oracle和Hive的对照则基本没看到过，本文也尝试给出。1 Data Type1.1 字符类型类型OracleHiveImpalaCHAR(n)n默认1字节，最大为2000，n可省略最大255同hiveNCHAR(n)n默认1字节，最大为2000，n可省略，存UnicodeNVARCHAR2(n)n不可省略，最大4000，存UnicodeVARCHAR2(n)n不可省略，最大4000VARCHAR(n)同VARCHAR2，不建议使用最大65535同hiveSTRING‘’或“”括住，文档中未说明最大长度，怀疑和Java的String相同，即2G个字符数，占4G字节（未测过）‘’或“”括住，最大32767字节1.2 数字类型类型OracleHiveImpalaNUMBER(p[,s])1-22字节，p取值范围1到38，s取值范围-84到127存储定点数，值的绝对值范围为1.0 x 10e-130至1.0 x 10e126。值大于等于1.0 x 10e126时报错。p为有意义的10进制位数，正值s为小数位数，负值s表示四舍五入到小数点左部多少位。BINARY_FLOAT5字节，其中有一长度字节。32位单精度浮点数类型。符号位1位，指数位8位，尾数位23位。BINARY_DOUBLE9字节，其中有一长度字节。64位双精度浮点数类型。DECIMAL(p[,s])只在语法上支持，底层实际就是NUMBER，无任何区别最大38位精度0.11引入时固定38位0.13开始允许自定义p和s最大38位精度TINYINT1-byte signed integer, from&nbsp;-128&nbsp;to&nbsp;127同hiveSMALLINT2-byte signed integer, from&nbsp;-32,768&nbsp;to&nbsp;32,767同hiveINT4-byte signed integer, from&nbsp;-2,147,483,648&nbsp;to&nbsp;2,147,483,647同hiveBIGINT8-byte signed integer, from&nbsp;-9,223,372,036,854,775,808&nbsp;to&nbsp;9,223,372,036,854,775,807同hiveFLOAT4-byte single precision floating point number同hiveDOUBLE8-byte double precision floating point number同hive1.3 时间类型&nbsp; &nbsp; &nbsp;这块还是很麻烦的，主要是除了类型外还有很多转换函数，这里不可能一一列举了，建议参考官方文档看。类型OracleHiveImpalaDATE7字节默认值为SYSDATE的年、月，日为01。包含一个时间字段，若插入值没有时间字段，则默认值为：00:00:00 or 12:00:00 for 24-hour and 12-hour clock time。没有分秒和时区。代表特定的年/月/日，格式为YYYY-&shy;MM-&shy;DD，可与Date, Timestamp, String互相转换TIMESTAMP从Unix纪元开始的时间间隔，以纳秒为精度，不考虑时区。如需转换为特定时区需要用UDF同HiveTIMESTAMP [(fractional_seconds_precision)]7至11字节fractional_seconds_precision为Oracle存储秒值小数部分位数，默认为6，可选值为0到9。没有时区。TIMESTAMP [(fractional_seconds_precision)] WITH TIME ZONE13字节使用UTC，包含字段YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, TIMEZONE_HOUR, TIMEZONE_MINUTETIMESTAMP [(fractional_seconds_precision)] WITH LOCAL TIME ZONE7至11字节存时使用数据库时区，取时使用会话的时区。INTERVAL YEAR [(year_precision)] TO MONTH5字节包含年、月的时间间隔类型。year_precision是年字段的数字位数，默认为2，可取0至9。INTERVAL DAY [(day_precision)]TO SECOND [(fractional_seconds_precision)]11字节day_precision是月份字段的数字位数，默认为2，可取0至9。1.4 其他类型&nbsp; &nbsp; &nbsp;基本都是某种库特有的。同类的进行合并类型OracleHiveImpalaBLOB,CLOB,NCLOB最大为(4GB-1)*数据库块大小。用来存储大字符串或二进制对象。BFILE最大为2 32-1字节。LOB地址指向文件系统上的一个二进制文件，维护目录和文件名。不参与事务处理。只支持只读操作。LONG, LONG RAW最大为2GB，变长类型，不建议使用RAW(n)最大2000字节，n为字节数不可省略，变长类型ROWIDUROWID(n)ROWID10字节，代表记录的地址。显示为18位的字符串。用于定位数据库中一条记录的一个相对唯一地址值。通常情况下，该值在该行数据插入到数据库表时即被确定且唯一。UROWID除存储物理地址外，还能存储逻辑或外来地址BOOLEANtrue或false同hiveBINARY存储二进制对象ARRAY&lt;data_type&gt;MAP&lt;primitive_type, data_type&gt;STRUCT&lt;col_name : data_type [COMMENT col_comment], …&gt;UNIONTYPE&lt;data_type, data_type, …&gt;四种复合类型，意义很明显其中对UNIONTYPE的支持是不完整的2 DDL&nbsp; &nbsp; &nbsp;从这里开始涉及到各系统SQL的语法细节，这里不会给出所有细节，如果要细查的话只有一种办法就是查看对应系统的手册。由于Oracle DBA是一门繁琐的学问，有无数个参数可以调，这里基本不会涉及，只对比数据分析人员常用的语法。（这是考虑到，虽然Hive/Impala也有不少选项可以设，但基本都是以配置文件的方式定义的，在SQL层面定义的很少，这与Oracle有着明显的不同，如果Oracle SQL的所有细节都写，这篇文章80%的篇幅都将是Oracle，这不是我想要的效果）2.1&nbsp;Create/Drop/Alter/Use Database操作OracleHiveImpalaCREATE有，参数很多很繁琐，另外只能写create database而不能写create schemaCREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name&nbsp; [COMMENT database_comment]&nbsp; [LOCATION hdfs_path]&nbsp; [WITH DBPROPERTIES (property_name=property_value, …)];CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name&nbsp; [COMMENT database_comment]&nbsp; [LOCATION hdfs_path];DROP有DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];RESTRICT是默认的，表示删除库前需手工删除所有表；CASCADE表示hive会帮你把库里的所有表都删了DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;ALTER有，参数很多很繁琐ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …);ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;USE通过database.tableuse&nbsp;database/schema或database.table另外有个叫default的默认schema同hive&nbsp; &nbsp; &nbsp;然后，Oracle有tablespace，segment等一堆概念，hive和impala都是没有的，所以相应操作SQL就不列了。2.2&nbsp;Create/Drop/Truncate Table操作OracleHiveImpalaCREATE语法略，参数很多很繁琐，值得注意的几点：1，有PCTFREE, TABLESPACE等参数用来设定存储细节2，分区方式有range分区，hash分区，list分区，composite分区四种3，RECOVERABLE参数可控制该表的变动是否写redo log语法略，值得注意的几点：1，TEMPORARY关键字用来建临时表（0.14.0以后），只对当前session可见，1.1.0后还可指定保存在mem，ssd或默认存储2，EXTERNAL指建外部表，很常用3，PARTITIONED BY只有哈希分区一种，且要指定字段类型4，可指定行格式，存储格式，SerDe5，SKEWED BY用来指定有偏斜数据的列，可加速查询6，CLUSTERED BY用来指定分桶，桶数可指定，还能排序7，还有种CREATE TABLE LIKE语法用于根据现有表的模式建表大致和Hive相同，不同的几点：1，无TEMPORARY2，无SKEWED BY3，无CLUSTERED BY4，CACHED IN指示利用利用HDFS caching特性将表目录下的数据预先加载进内存，以加速查询5，CREATE TABLE LIKE语法和hive不一样DROP有，也有purge参数DROP TABLE [IF EXISTS] table_name [PURGE];PURGE是0.14.0后新增的，带上这个意味着如果删除托管表，HDFS上的文件被删除且不进回收站。外部表drop时该参数不起作用。DROP TABLE [IF EXISTS] [db_name.]table_nameTRUNCATE有，但不能删除某个分区的数据，要删分区数据必须alter table table_name truncate…TRUNCATE TABLE table_name [PARTITION partition_spec];可用来删分区数据，这和Oracle不同2.3&nbsp;Alter Table/Partition/Column操作OracleHiveImpalaALTER TABLE语法略，除下述partition和column外，还有以下子句：alter_table_propertiesconstraint_clausesalter_external_tablemove_table_clauseenable_disable_clause建议参考官方手册语法略，除下述partition和column外，还有：alter table/SerDe/storage properties;alter skewed or Stored as Directories;总体而言和Oracle的差异程度相当于CREATE的差异程度，区别巨大类似hive，但无SKEWED BY，无CLUSTERED BY，有CACHED IN，差异程度相当于CREATE的差异程度ALTER TABLE … PARTITION该类子句包含{&nbsp;modify_table_default_attrs|alter_interval_partitioning|set_subpartition_template|modify_table_partition|modify_table_subpartition|&nbsp;move_table_partition|move_table_subpartition|add_table_partition|coalesce_table_partition|coalesce_table_subpartition|drop_table_partition|drop_table_subpartition|rename_partition_subpart|truncate_partition_subpart|split_table_partition|split_table_subpartition|merge_table_partitions|merge_table_subpartitions|exchange_partition_subpart}总体上来说，Oracle中分区的方式和操作都和hive有很大区别，建议参考官方手册语法略，包含：ADD, RENAME, EXCHANGE, RECOVER, DROP, (UN)ARCHIVE等操作另外，还有几种Table/Partition的复合操作建议参考官方手册类似hive，但纯Partition操作只有ADD, DROPTable/Partition的复合操作有SET PROPERTY和SET CACHED IN两种比hive简化一些，建议参考官方手册ALTER TABLE … COLUMN{&nbsp;{&nbsp;add_column_clause&nbsp; | modify_column_clauses&nbsp; |&nbsp;drop_column_clause&nbsp; }…|&nbsp;rename_column_clause|&nbsp;{&nbsp;modify_collection_retrieval&nbsp;}…|&nbsp;{&nbsp;modify_LOB_storage_clause&nbsp;}…|&nbsp;{&nbsp;alter_varray_col_properties&nbsp;}…}建议参考官方手册语法略，包含改变列的名称、类型、位置、注释，增加、替换列等操作。其中，从0.14.0开始支持对某个分区单独操作；从0.15.0开始支持CASCADE|RESTRICT关键字，其含义和前面所述的相同关键字一致。建议参考官方手册类似hive，支持对列的ADD, REPLACE, CHANGE, DROP（hive没有）操作。但是不支持对分区内的列单独操作，也没有那两个特殊关键字。建议参考官方手册2.4&nbsp;Create/Drop/Alter View操作OracleHiveImpalaCREATECREATE [OR REPLACE]&nbsp; [[NO] FORCE] [EDITIONING] VIEW [schema.] view&nbsp; &nbsp;[ ( { alias [ inline_constraint… ]&nbsp; &nbsp; &nbsp; &nbsp;| out_of_line_constraint&nbsp; &nbsp; &nbsp; &nbsp;}&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[, { alias [ inline_constraint…]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | out_of_line_constraint&nbsp; &nbsp; &nbsp;}&nbsp; ]&nbsp; &nbsp; &nbsp;)&nbsp; &nbsp;| object_view_clause&nbsp; &nbsp;| XMLType_view_clause&nbsp; &nbsp;]&nbsp; &nbsp;AS subquery [ subquery_restriction_clause ] ;以上是创建普通视图语句，比较常用的关键字是WITH CHECK OPTION指定修改时检查约束，WITH READ ONLY指定只读。另外Oracle中可以创建物化视图，CREATE MATERIALIZED VIEW，具体语法从略CREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], …) ]&nbsp; [COMMENT view_comment]&nbsp;[TBLPROPERTIES (property_name = property_value, …)]&nbsp; AS SELECT …;hive的建视图语法和oracle也有较大差别，最大差异是hive的视图都是只读的，并且没有物化视图，其余语法差异可参考官方手册CREATE VIEW [IF NOT EXISTS] view_name [(column_list)]&nbsp; AS select_statement;和hive大致相同，除了没有COMMENT和TBLPROPERTIESDROPDROP VIEW [ schema. ] view [ CASCADE CONSTRAINTS ] ;CASCADE CONSTRAINTS用来删掉主键约束，不带的话如有约束会失败DROP VIEW [IF EXISTS] [db_name.]view_name;同hiveALTER VIEWALTER VIEW [ schema. ] view&nbsp; { ADD out_of_line_constraint&nbsp; | MODIFY CONSTRAINT constraint&nbsp; &nbsp; &nbsp; { RELY | NORELY }&nbsp; | DROP { CONSTRAINT constraint&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| PRIMARY KEY&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| UNIQUE (column [, column ]…)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}&nbsp; | COMPILE&nbsp; | { READ ONLY | READ WRITE }&nbsp; } ;可见，Oracle的ALTER VIEW主要是用来修改视图的约束以及读写权限的ALTER VIEW [db_name.]view_name SET TBLPROPERTIES table_properties;相比之下，hive的ALTER VIEW完全不同，只能修改建view时指定的元数据ALTER VIEW [database_name.]view_name RENAME TO [database_name.]view_nameImpala的ALTER VIEW也不一样，只能修改视图名，其他什么都不能改ALTER VIEW AS SELECTALTER VIEW [db_name.]view_name AS select_statement;注意Oracle没有这种语法。同hive2.5&nbsp;Create/Drop/Alter Index操作OracleHiveImpalaCREATECREATE [ UNIQUE | BITMAP ] INDEX [ schema. ] index&nbsp; ON { cluster_index_clause&nbsp; &nbsp; &nbsp;| table_index_clause&nbsp; &nbsp; &nbsp;| bitmap_join_index_clause&nbsp; &nbsp; &nbsp;}[ UNUSABLE ] ;Oracle的索引非常复杂，至少支持Normal(B-tree), Bitmap, Partitioned, Function-based, Domain五种类型。每一种都有复杂的语法，建议参考官方手册CREATE INDEX index_name&nbsp; ON TABLE base_table_name (col_name, …)&nbsp; AS index_type&nbsp; [WITH DEFERRED REBUILD]&nbsp; [IDXPROPERTIES (property_name=property_value, …)]&nbsp; [IN TABLE index_table_name]&nbsp; [PARTITIONED BY (col_name, …)]&nbsp; [&nbsp; &nbsp; &nbsp;[ ROW FORMAT …] STORED AS …&nbsp; &nbsp; &nbsp;| STORED BY …&nbsp; ]&nbsp; [LOCATION hdfs_path]&nbsp;[TBLPROPERTIES (…)]&nbsp; [COMMENT “index comment”];Hive的索引与Oracle也非常不同，首先默认的索引不是B-tree的而是元数据库中的汇总表，因此可以用IN TABLE子句指定存储的表名。从0.8.0起对取值很少的列还支持Bitmap索引。分区的写法和用法也和Oracle不同。DROPDROP INDEX [ schema. ] index [ FORCE ] ;Oracle的索引在drop时是不需要指定表名的DROP INDEX [IF EXISTS] index_name ON table_name;Hive则需要指定表名，因为Hive里不同表的索引可以重名ALTERALTER INDEX [ schema. ]index&nbsp; { { deallocate_unused_clause&nbsp; &nbsp; | allocate_extent_clause&nbsp; &nbsp; | shrink_clause&nbsp; &nbsp; | parallel_clause&nbsp; &nbsp; | physical_attributes_clause&nbsp; &nbsp; | logging_clause&nbsp; &nbsp; } …&nbsp; | rebuild_clause&nbsp; | PARAMETERS ( ‘ODCI_parameters’ )&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)&nbsp; | COMPILE&nbsp; | { ENABLE | DISABLE }&nbsp; | UNUSABLE&nbsp; | VISIBLE | INVISIBLE&nbsp; | RENAME TO new_name&nbsp; | COALESCE&nbsp; | { MONITORING | NOMONITORING } USAGE&nbsp; | UPDATE BLOCK REFERENCES&nbsp; | alter_index_partitioning&nbsp; }&nbsp;;Oracle对索引的修改也非常复杂，比较常用的如对分区索引可以单独修改/删除，各种语法建议参考官方手册ALTER INDEX index_name ON table_name [PARTITION partition_spec] REBUILD;Hive的ALTER INDEX极其简单，只有一种功能就是重建索引，当然如果是分区索引可以对某分区单独重建2.6 其他&nbsp; &nbsp; &nbsp;还有很多DDL是Oracle特有的（比如Sequence，Trigger之类）或Hive特有的（如Macro），因为用不着对比就不列了。还有角色、权限之类的语句因为通常不关心也不列。 3 DML3.1 Load Files操作OracleHiveImpalaLOADLOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]同hive3.2 Insert操作OracleHiveImpalaINSERTINSERT [ hint ]&nbsp; &nbsp;{ single_table_insert | multi_table_insert } ;single_table_insert ::=insert_into_clause{&nbsp;values_clause&nbsp;[&nbsp;returning_clause&nbsp;]|&nbsp;subquery&nbsp;}&nbsp;[&nbsp;error_logging_clause&nbsp;]multi_table_insert ::={ ALL&nbsp; { insert_into_clause [ values_clause ] [error_logging_clause] }…| conditional_insert_clause} subqueryOracle单表的values和subquery插入和其他两种差不多，多表插入是其他两种没有的INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement;INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] …)] VALUES values_row [, values_row …]hive的INSERT VALUES从0.14.0开始才有[with_clause]INSERT { INTO | OVERWRITE } [TABLE] table_name&nbsp; [(column_list)]&nbsp; [ PARTITION (partition_clause)]{&nbsp; &nbsp; [hint_clause] select_statement&nbsp; | VALUES (value [, value …]) [, (value [, value …]) …]}基本和hive一致，除了头部可以带with子句3.3 Update功能OracleHiveImpalaUPDATEUPDATE [ hint ]&nbsp; &nbsp;{ dml_table_expression_clause&nbsp; &nbsp;| ONLY (dml_table_expression_clause)&nbsp; &nbsp;} [ t_alias ]&nbsp;update_set_clause&nbsp; &nbsp;[ where_clause ]&nbsp; &nbsp;[ returning_clause ]&nbsp;[error_logging_clause] ;建议参考官方手册UPDATE tablename SET column = value [, column = value …] [WHERE expression]从0.14.0开始才有，存在一些限制，比如赋的值必须来自于表达式而不是子查询，不支持分区、分桶的列等3.4 Delete功能OracleHiveImpalaDELETEDELETE [ hint ]&nbsp; &nbsp;[ FROM ]&nbsp; &nbsp;{ dml_table_expression_clause&nbsp; &nbsp;| ONLY (dml_table_expression_clause)&nbsp; &nbsp;} [ t_alias ]&nbsp; &nbsp; &nbsp;[ where_clause ]&nbsp; &nbsp; &nbsp;[ returning_clause ]&nbsp;[error_logging_clause];与UPDATE高度相似，建议参考官方手册DELETE FROM tablename [WHERE expression];从0.14.0开始才有，值得注意的是hive的DELETE（UPDATE也是）只要语句成功执行就自动提交3.5 其他功能OracleHiveImpalaMERGEMERGE [ hint ]&nbsp; &nbsp;INTO [ schema. ] { table | view } [ t_alias ]&nbsp; &nbsp;USING { [ schema. ] { table | view }&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| subquery&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} [ t_alias ]&nbsp; &nbsp;ON ( condition )&nbsp; &nbsp;[ merge_update_clause ]&nbsp; &nbsp;[ merge_insert_clause ]&nbsp; &nbsp;[ error_logging_clause ];Oracle提供的将一个表上的多个INSERT,UPDATE,DELETE操作合并在一起的简便语法，非常实用，建议参考官方手册&nbsp; &nbsp; &nbsp;其他一些不常用的或者独有的就不列了。4 Queries4.1基本SELECT功能OracleHiveImpalaSELECT … WHERE语法略，WHERE子句的写法和hive是相同的建议参考官方手册[WITH CommonTableExpression (, CommonTableExpression)] &nbsp;SELECT [ALL | DISTINCT] select_expr, select_expr, …&nbsp; FROM table_reference&nbsp; [WHERE where_condition]&nbsp; [GROUP BY col_list]&nbsp; [ORDER BY col_list]&nbsp; [CLUSTER BY col_list&nbsp; &nbsp; | [DISTRIBUTE BY col_list] [SORT BY col_list]&nbsp; ]&nbsp;[LIMIT number]WHERE子句的写法没有大区别基本等同与hive，CLUSTER BY,&nbsp;DISTRIBUTE BY,&nbsp;SORT BY没有DISTINCT/ALL/UNIQUESELECT后可接DISTINCT，UNIQUE（等同DISTINCT），ALL（全部返回，默认）三种关键字有DISTINCT，ALL只有DISTINCTGROUP BY/HAVINGgroup_by_clause ::=GROUP BY&nbsp; &nbsp;{ expr&nbsp; &nbsp;| rollup_cube_clause&nbsp; &nbsp;| grouping_sets_clause&nbsp; &nbsp;}&nbsp; &nbsp; &nbsp;[, { expr&nbsp; &nbsp; &nbsp; &nbsp; | rollup_cube_clause&nbsp; &nbsp; &nbsp; &nbsp; | grouping_sets_clause&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp; &nbsp; &nbsp;]…&nbsp; &nbsp;[ HAVING condition ]Oracle中ROLLUP, CUBE, GROUPING SETS这些BI中常用的功能GROUP BY groupByExpression (, groupByExpression)也支持ROLLUP, CUBE, GROUPING SETS，语法和Oracle有细微差别只支持最基本的用法LIMIT无（只能通过rownum限制结果集行数）有有4.2 Sort功能OracleHiveImpalaORDER BYorder_by_clause ::=ORDER [ SIBLINGS ] BY{ expr | position | c_alias }[ ASC | DESC ][ NULLS FIRST | NULLS LAST ]&nbsp; [, { expr | position | c_alias }&nbsp; &nbsp; &nbsp;[ ASC | DESC ]&nbsp; &nbsp; &nbsp;[ NULLS FIRST | NULLS LAST ]&nbsp; ]…SIBLINGS关键字仅当使用了CONNECT BY时才有用；另一个特点是，Oracle可以按表达式/列序号/列名三种之一排序，而其他两种都只支持列名ORDER BY colName [ASC | DESC] (‘,’ colName [ASC | DESC])hive很常规ORDER BY col_ref [, col_ref …] [ASC | DESC] [NULLS FIRST | NULLS LAST]NULLS FIRST | NULLS LAST用于指定null排在结果集的头部还是尾部SORT BY仅hive支持，和ORDER BY区别是，它只对每个reducer内的数据排序，因此最后汇总后的数据可能不是严格排序的4.3 Join&nbsp; &nbsp; &nbsp;Join可以说是差别最大的语法之一，因为对效率的影响最大，所以三种库都有不同的折衷和妥协。强烈建议看官方手册。功能OracleHiveImpalaJOINjoin_clause ::=table_reference&nbsp; { inner_cross_join_clause | outer_join_clause }…inner_cross_join_clause::={ [ INNER ] JOIN table_reference&nbsp; &nbsp; { ON condition&nbsp; &nbsp; | USING (column [, column ]…)&nbsp; &nbsp; }| { CROSS&nbsp; | NATURAL [ INNER ]&nbsp; }&nbsp; JOIN table_reference}outer_join_clause::=&nbsp; [ query_partition_clause ] [ NATURAL ]outer_join_type JOIN table_reference&nbsp; [ query_partition_clause ]&nbsp; [ ON condition&nbsp; | USING ( column [, column ]…)&nbsp; ]值得注意的点：1，只有它支持非等值连接，即condition可以是任意条件表达式2，只有它支持自然连接3，支持USING指定同名列的语法4，不支持半连接，必须用EXISTS替代5，不支持反连接，必须用NOT IN替代table_reference JOIN table_factor [join_condition]| table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition| table_reference LEFT SEMI JOIN table_reference join_condition| table_reference CROSS JOIN table_reference [join_condition]值得注意的点：1，hive仅支持等值连接，因为非等值连接用map/reduce范式无法实现2，和Oracle相比直接支持半连接3，反连接还是不支持4，USING语法不支持5，可用/+ STREAMTABLE(a) / 指示流表，hive连接的时候，默认会将最右端的表放进流中用来驱动，其余表全部读进内存中，因此内存容易爆。解决办法是将最大的表写在最右，或者用这个hint来指定最大的表为流表6，可用/+ MAPJOIN(b) */ 指示map端连接。这个也有三种用法：一表特小，可使用；所有表join的列都是分桶的，可使用；所有表join的列都是分桶且排序的，可使用（称为sort-merge join）SELECT select_list FROM&nbsp;table_or_subquery1 [INNER] JOIN table_or_subquery2 |&nbsp;table_or_subquery1 {LEFT [OUTER] | RIGHT [OUTER] | FULL [OUTER]} JOIN table_or_subquery2 |&nbsp;table_or_subquery1 {LEFT | RIGHT} SEMI JOIN table_or_subquery2 |&nbsp;table_or_subquery1 {LEFT | RIGHT} ANTI JOIN table_or_subquery2 |&nbsp; &nbsp; [ ON col1 = col2 [AND col3 = col4 …] |&nbsp; &nbsp; &nbsp; USING (col1 [, col2 …]) ]&nbsp; [other_join_clause …][ WHERE where_clauses ];值得注意的点：1，这种语法只支持等值连接2，支持USING指定同名列的语法3，支持半连接4，支持反连接5，不支持自然连接＝/&gt;=/&lt;=… &nbsp;(+)Oracle支持这种在where中写条件进行连接的方式（支持等值与非等值），并用(+)指示外连接，虽然用的人很多，但并不推荐SELECT select_list FROM&nbsp;table_or_subquery1, table_or_subquery2 [, table_or_subquery3 …]&nbsp; [other_join_clause …]WHERE&nbsp; &nbsp; col1 = col2 [AND col3 = col4 …];SELECT select_list FROM&nbsp;table_or_subquery1 CROSS JOIN table_or_subquery2&nbsp; [other_join_clause …][ WHERE where_clauses ];Impala也支持where中连接，但第一种写法只支持等值连接，如果要使用非等值连接只能用CROSS JOIN。另外也不支持外连接4.4 集合操作功能OracleHiveImpalaUNION写UNION就是去重的，没有DISTINCT写法写UNION就是不去重select_statement UNION [ALL | DISTINCT] select_statement …hive1.2.0后才支持UNION DISTINCT同hiveINTERSECT/MINUS支持4.5 Subqueries功能OracleHiveImpala子查询用途很广：1，INSERT或CREATE TABLE语句中定义行集2，CREATE VIEW或CREATE METERIALIZED VIEW语句中定义行集3，UPDATE语句中定义要赋的一个或多个值4，WHERE段、HAVING段、或SELECT/UPDATE/DELETE语句的START WITH段中提供条件5，通过包一个查询来提供要操作的表，比如SELECT语句的FROM段，或INSERT/UPDATE/DELETE语句的表名处SELECT … FROM (subquery) name …SELECT … FROM (subquery) AS name …hive只在FROM段中支持子查询SELECT select_list FROM table_ref [, table_ref …]WHERE value comparison_operator (scalar_select_statement)WHERE value [NOT] IN (select_statement)WHERE [NOT] EXISTS (correlated_select_statement)WHERE NOT EXISTS (correlated_select_statement)支持FROM段和WHERE段中的子查询4.6 其他功能OracleHiveImpaladual有有REGEXSELECT后的列名支持正则表达式&nbsp; &nbsp; &nbsp;还有很多暂时没想到。 5 Function&nbsp; &nbsp; &nbsp;指数据库内置的function，不讨论UDF。另外，操作符都不比较了，区别不大。5.1 数学函数功能OracleHiveImpalaABS绝对值，有有有SIN/SINH/ASIN/COS/COSH/ACOS/TAN/TANH/ATAN/ATAN2三角函数其中ATAN2接受两个参数（二维平面中的坐标）没有SINH/COSH/TANH/ATAN2同hiveBITAND按位与，有CEIL天花板值，有有，还有个别名CEILING有，同hiveEXPe的多少次，有有，还有个函数E()返回e有，同hiveFLOOR地板值，有有有LN以e为底的log，有有有LOG以某个double为底的log，有有，还有两个特殊底的log：LOG2和LOG10有，同hiveMODOracle的MOD的计算方式为MOD(n2,n1)=n2 - n1 FLOOR(n2/n1)，这与经典取模还不同，比如Oracle的MOD(-11,4)=-3，但经典取模等于1在hive里取模用PMOD，返回值一定是个正数，比如PMOD(-11,4)=1。但这与经典取模还是不同，比如PMOD(-11,-4)=1，但经典取模等于-3Impala的PMOD和hive的PMOD相同，另外Impala还有个FMOD和Oracle的MOD相同POWER求幂，有有，还有个别名POW有，同hiveREMAINDER取余，REMAINDER(n2,n1)=n2 - n1 FLOOR(n2/n1)ROUND舍入，支持1个参数和2个参数两种版本有另外还有一个BROUND，使用HALF_EVEN舍入模式，见官方手册有，同OracleSIGN符号函数，有有有SQRT开方，有有有TRUNC截取数值的小数点后多少位（如果是负值则往前推）DEGREES/RADIANS角度/弧度互转同hivePOSITIVE/NEGATIVE相当于在数值前加+/-号（因此没什么卵用）同hivePI返回pi值同hiveFACTORIAL阶乘，1.2.0以后才有CBRT求立方根，1.2.0以后才有SHIFTLEFT/SHIFTRIGHT/SHIFTRIGHTUNSIGNED按位左移/右移/无符号右移GREATEST/LEAST返回一串值中的最大/最小值，这串值的类型可以是任意，只要可比较大小（所以其实不是数学函数而是通用比较）有，1.1.0以后有5.2 字符（串）函数功能OracleHiveImpalaASCII输入必须char，返回该字符的ascii数值输入String，返回该String第一个字符的ascii数值同hiveBASE64 / UNBASE64将二进制值转为base64的String（UN则是反向）CONCATCONCAT(char1, char2)char或char2均可为CHAR,VARCHAR2,NCHAR,NVARCHAR2,CLOB, or NCLOB之一concat(string|binary A, string|binary B…)可以有多个参数另外提供CONCAT_WS用于指定特殊分隔符的连接同hive（除了不支持binary类型），且也有CONCAT_WSDECODE/ENCODEstring decode(binary bin, string charset)&nbsp;binary encode(string src, string charset)编码和解码用的，用于支持hive特有的binary类型（其实oracle和impala也有DECODE，但作用完全不同，见条件函数中的DECODE）FIND_IN_SETfind_in_set(string str, string strList)strList是用’,’分割的一组string，该函数将寻找strList中第一个精确匹配的str同hiveFORMAT_NUMBER将数字格式化为stringGET_JSON_OBJECT抽取JSON对象，不常用IN_FILEin_file(string str, string filename)检测str是否为filename对应文件中的某行，不常用INITCAP将每个单词（以空白分隔）转换为首字母大写其余小写的形式同oracle，1.1.0开始有同oracleINSTR{ INSTR| INSTRB| INSTRC| INSTR2| INSTR4}(string , substring [, position [, occurrence ] ])搜索子串，不同数据类型调用名不同，最多可带四个参数，其中第三个是开始位置，第四个是出现的第几次instr(string str, string substr)只接受两个参数同hiveLENGTH{ LENGTH| LENGTHB| LENGTHC| LENGTH2| LENGTH4}(char)串长，不同数据类型调用名不同有（仅LENGTH）同hiveLEVENSHTEIN返回两个串的Levenshtein距离（编辑距离）1.2.0后才有LOCATE特殊情况的INSTR，可以指定匹配的开始位置。Oracle因为本来INSTR就支持所以并不需要该函数同hiveLOWER转小写有，且有一个别名叫LCASE同hiveLPAD / RPADLPAD(expr1, n [, expr2 ])在expr1之前用expr2填充n个字符，如expr2省略默认用n个单空格填充。RPAD类似只是在右边有，但expr2不能省略同hiveLTRIM / RTRIMLTRIM(char [, set ])去掉char左侧包含在set中的字符，如省略set，则只去除空格符。RTRIM类似只是在右边有，但没有set参数，即只能去除空格符同hivePARSE_URL抽取URL，可以指定抽取URL的类型和建名同hivePRINTF按格式打印对象数组REGEXP_EXTRACT / REGEXP_SUBSTRREGEXP_SUBSTR(source_char, pattern&nbsp;[, position&nbsp; [, occurrence&nbsp; &nbsp;[, match_param&nbsp; &nbsp; [, subexpr] &nbsp;] &nbsp;] &nbsp;] &nbsp;)按正则表达式抽取字符串，并返回其中一部分。注意Oracle和hive/impala中函数名不同regexp_extract(string subject, string pattern, int index)类似Oracle，但不能指定起始位置，替换序号和匹配参数同hiveREGEXP_REPLACEREGEXP_REPLACE(source_char, pattern&nbsp;[, replace_string&nbsp; &nbsp;[, position&nbsp; &nbsp; [, occurrence&nbsp; &nbsp; &nbsp;[, match_param ] &nbsp;] ] ] )按正则表达式替换字符串中的一部分regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)类似Oracle，但不能指定起始位置，替换序号和匹配参数同hiveREGEXP_COUNTREGEXP_COUNT (source_char, pattern [, position [, match_param]])按正则表达式对字符串中子串的出现次数计数REGEXP_INSTRREGEXP_INSTR (source_char, pattern&nbsp;[, position&nbsp; [, occurrence&nbsp; &nbsp;[, return_opt&nbsp; &nbsp; [, match_param&nbsp; &nbsp; &nbsp;[, subexpr]&nbsp; ]&nbsp;]&nbsp;]&nbsp;] )在字符串中查找满足正则表达式的子串第一次出现的位置REPEAT重复n次同hiveREPLACE简单替换文本REVERSE返回逆串同hiveSENTENCES简单的分词功能，很奇怪hive为何提供这样的功能SOUNDEX取英文中的“谐音”，可能是用于存在脏数据时的匹配吧，诡异的功能有，1.2.0以后SPACE返回n个空格同hiveSPLIT使用正则表达式分割字符串STR_TO_MAP将字符串转为键值对STRLEFT / STRRIGHT返回最左/最右的n个字符，是SUBSTR的简化版SUBSTR / SUBSTRING{ SUBSTR| SUBSTRB| SUBSTRC| SUBSTR2| SUBSTR4}(char, position [, substring_length ])不同数据类型调用名不同类似Oracle，只有SUBSTR和其别名SUBSTRING同hiveSUBSTRING_INDEXsubstring_index(string A, string delim, int count)返回A在delim出现第count次前的部分，1.3.0后才有TRANSLATETRANSLATE(expr, from_string, to_string)将expr，按from_string中出现的每个字符替换为to_string中对应序号的字符Oracle还有种TRANSLATE…USING语法是其他两个没有的同oracle同oracleTRIMTRIM([ { { LEADING | TRAILING | BOTH }&nbsp;[ trim_character ]&nbsp; &nbsp; &nbsp; &nbsp;| trim_character}&nbsp;FROM&nbsp;&nbsp;]trim_source)比较灵活，可指定去除前端还是后端，去除什么字符。如果只留trim_source一个参数则等同于后两者trim(string A)简单去除A前后的空白同hiveUPPER转大写有，且有一个别名LCASE同hive5.3 日期函数功能OracleHiveImpalaADD_MONTHS在某日期上加上n个月有，1.1.0以后有CURRENT_DATE返回当前时间（和session的时区相关），精确到秒返回当前时间（sql评估时的时间，同一个查询中多次调用该函数值相同），1.2.0以后CURRENT_TIMESTAMP返回当前时间（和session的时区相关），精确到毫秒，返回类型为TIMESTAMP WITH TIME ZONE返回当前时间（sql评估时的时间，同一个查询中多次调用该函数值相同，精确到毫秒），1.2.0以后有，另有一个别名NOWDATE_ADD / DATE_SUB在某日期上加/减n天同hive，可接收timestamp或string类型。只接收timestamp类型的该函数有两套，称为DAYS_ADD/DAYS_SUB，ADDDATE/SUBDATE（真不懂impala搞这么多名字雷同的东西干啥）DATE_FORMAT用格式字符串格式化日期（可为date/timestamp/string）DATE_PART省略order参数的EXTRACTDATEDIFF求两个日期间差的天数同hiveDAY / DAYOFMONTH返回该日期在月内的日数，两个函数同义同hiveDAYNAME返回周间的名字，即’Sunday’到’Saturday’DAYOFWEEK返回周间的序号，1(Sunday)到7(Saturday)DAYOFYEAR返回是本年第几天DBTIMEZONE数据库当前时区EXTRACTEXTRACT( { YEAR&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| MONTH&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| DAY&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| HOUR&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| MINUTE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| SECOND&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| TIMEZONE_HOUR&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| TIMEZONE_MINUTE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| TIMEZONE_REGION&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| TIMEZONE_ABBR&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FROM { expr }&nbsp;)按参数提取日期中的某部分extract(timestamp, string unit)extract(unit FROM timestamp)比Oracle多一种格式FROM_TZ将时间戳和时区合并为带时区的时间戳FROM_UNIXTIME将unix纪元以来的秒数转化为时间字符串同hiveFROM_UTC_TIMESTAMP将UTC的时间戳值转化为指定时区的时间戳值同hiveHOUR返回时间字符串的小时值同hiveHOURS_ADD / HOURS_SUB在某日期上加/减n个小时LAST_DAY返回该日期所在月份的最后一天同Oracle，1.1.0以后LOCALTIMESTAMP返回当前时间（和session的时区相关），精确到毫秒，返回类型为TIMESTAMPMICROSECONDS_ADD /&nbsp;MICROSECONDS_SUB在某日期上加/减n微秒MILLISECONDS_ADD / MILLISECONDS_SUB在某日期上加/减n毫秒MINUTE返回时间字符串的分钟值MINUTES_ADD /&nbsp;MINUTES_SUB在某日期上加/减n分钟MONTH返回时间字符串的月份同hiveMONTHS_ADD /&nbsp;MONTHS_SUB在某日期上加/减n个月（其实第一个和ADD_MONTHS重复）MONTHS_BETWEEN返回两个日期间相差的月数，注意返回值是个浮点数同Oracle，1.2.0后NANOSECONDS_ADD /&nbsp;NANOSECONDS_SUB在某日期上加/减n纳秒（impala搞这么多没用的加减函数真不知道干什么）NEW_TIME将时区1的时间转换为时区2的时间NEXT_DAY返回指定日期后下一个星期几的日期同Oracle，1.2.0后NUMTODSINTERVAL /&nbsp;NUMTOYMINTERVAL生成n时间单位的一个日期间隔。前一函数的间隔可选DAY,HOUR,MINUTE,SECOND，后一个的间隔可选MONTH,YEARQUATER返回日期的季度值（1-4），1.3.0后ROUND对日期做舍入SECOND返回时间字符串的秒值同hiveSECOND_ADD /&nbsp;SECOND_SUB在某日期上加/减n秒SESSIONTIMEZONE返回session的时区SYS_EXTRACT_UTC从日期字符串中抽取UTC日期SYSDATE / SYSTIMESTAMP返回操作系统日期，前者到秒，后者到微秒TO_CHAR将date或timestamp类型转换为varchar2，常用TO_DATE返回时间戳的日期部分同hiveTO_DSINTERVAL /&nbsp;TO_YMINTERVAL将一个字符串转换为INTERVAL DAY TO SECOND / INTERVAL YEAR TO MONTH类型的时间间隔TO_TIMESTAMP / TO_TIMESTAMP_TZ将一个字符串转换为时间戳，前一个不带时区，后一个带时区TO_UTC_TIMESTAMP将带时区的时间戳转换为UTC的同hiveTRUNC对日期做舍，语法类似ROUND，支持舍入到年、季度、月、周、日、小时、分钟等精度1.2.0后有，只支持舍入到年、月同oracleTZ_OFFSET返回某个时区和UTC间的偏差值UNIX_TIMESTAMP返回秒为单位的时间戳数值，无参数时为当前时间，一个参数时需要传入yyyy-MM-dd HH:mm:ss格式的时间字符串，两个参数时可以自定义传入时间格式同hiveWEEKOFYEAR返回该日期所在的周是年中第几周WEEKS_ADD /&nbsp;WEEKS_SUB在某日期上加/减n周YEAR返回该日期的年份同hiveYEARS_ADD / YEARS_SUB在某日期上加/减n年5.4 转换函数功能OracleHiveImpalaCASTCAST({ expr | MULTISET (subquery) } AS typename)输入可以是表达式也可以是集合cast(expr as &lt;type&gt;)将表达式转换为指定类型同hiveBINARY将参数转换为binary类型其余各种TO / TO都是Oracle特有的转换函数，建议看官方手册5.5 条件函数功能OracleHiveImpalaCASE … WHEN1，CASE a WHEN b THEN c [WHEN d THEN e] [ELSE f] ENDWhen a = b, returns c; when a = d, returns e; else returns f.2，CASE WHEN a THEN b [WHEN c THEN d] [ELSE e] ENDWhen a = true, returns b; when c = true, returns d; else returns e.同hive（注：因为在Oracle中如case…when是表达式，而hive和impala中这些是用函数来处理的，虽然提供了与Oracle相似的语法，但语言层面实现机制不同）COALESCE接收多个值，返回这些值中第一个非NULL的，如果全是NULL则返回NULL同oracle同oracleDECODEDECODE(expr, search, result [, search, result ]…&nbsp; [, default ])对expr，如果满足第一个search则返回第一个result，如果满足第二个search则返回第二个result同oracleIFif(boolean testCondition, T valueTrue, T valueFalseOrNull)testCondition如果真则返回valueTrue，如果假或NULL则返回valueFalseOrNull同hiveISNULLisnull(a)如果a为NULL返回true，否则返回falseisnull(type a, type ifNotNull)如果a非NULL则返回a，否则返回ifNotNull。注意和hive有重大区别，另该函数有别名IFNULL和NVLISNOTNULL和ISNULL相反LNNVLLNNVL(condition)如果condition为false或unknown返回true，如果为true返回falseNANVLNANVL(n2, n1)如果n2是NaN返回n1，否则返回n2NULLIFNULLIF(expr1, expr2)等价与CASE WHEN expr1 = expr2 THEN NULL ELSE expr1 END同oracleNULLIFZEROnullifzero(numeric_expr)如果numeric_expr为0返回NULL，否则返回该表达式的值NVLNVL(expr1, expr2)如果expr1为NULL则返回expr2，否则返回expr1同oracle同oracleNVL2NVL2(expr1, expr2, expr3)如果expr1非NULL则返回expr2，如果为NULL则返回expr3ZEROIFNULLzeroifnull(numeric_expr)如果numeric_expr为NULL返回0，否则返回该表达式的值5.6 聚合函数&nbsp; &nbsp; &nbsp;以上5类函数都是对单行操作的，接下去的两类：聚合函数和分析函数，则是跨行操作的。功能OracleHiveImpalaAPPX_MEDIANAPPX_MEDIAN([DISTINCT | ALL] expression)以抽样的方式，计算某列大致的中位数值AVGAVG([ DISTINCT | ALL ] expr) [ OVER(analytic_clause) ]OVER后可带分析函数子句同oracle同oracleCOLLECTCOLLECT( [ DISTINCT | UNIQUE ] column [ ORDER BY expr ] )该语句汇聚某列的值构造一张内嵌表COLLECT_SET / COLLECT_LIST将一组对象组成一个array，其中带SET的函数会去重，带LIST的函数不去重CORRCORR(expr1, expr2) [ OVER (analytic_clause) ]计算两列的皮尔逊相关系数，OVER后可带分析函数子句还有两个变种CORR_S和CORR_K没有OVER子句，也没有变种COUNTCOUNT({ | [ DISTINCT | ALL ] expr }) [ OVER (analytic_clause) ]OVER后可带分析函数子句同oracle同oracleCOVAR_POPCOVAR_POP(expr1, expr2)&nbsp; &nbsp;[ OVER (analytic_clause) ]计算总体协方差，OVER后可带分析函数子句没有OVER子句COVAR_SAMPCOVAR_SAMP(expr1, expr2) [ OVER (analytic_clause) ]计算样本协方差，OVER后可带分析函数子句没有OVER子句CUME_DIST计算一组数据的累积分布，有聚合和分析两种用法，详见官方手册DENSE_RANKDENSE_RANK(expr [, expr ]…) WITHIN GROUP&nbsp; (ORDER BY expr [ DESC | ASC ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[ NULLS { FIRST | LAST } ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [,expr [ DESC | ASC ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[ NULLS { FIRST | LAST } ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]…&nbsp; )和RANK的区别是，有并列值时下一位会继续编号，如两个值并列第1，下一个值排第2FIRST / LAST某数据集进行排序后，可对第一条/最后一条记录进行处理，详见官方手册GROUP_ID用于消除GROUP BY子句返回的重复记录GROUPING用于区分是数据库中本来的值还是汇聚后的值GROUPING_ID输入一列或多列，返回GROUPING位向量的十进制值GROUP_CONCAT将一列的值组合为一个stringHISTOGRAM_NUMERIC计算数值列的直方图LISTAGG将一列的值组合为一个string，可指定分组、排序等参数MAX / MINMAX([ DISTINCT | ALL ] expr) [ OVER (analytic_clause) ]OVER后可带分析函数子句同oracle同oracleMEDIANMEDIAN(expr) [ OVER (query_partition_clause) ]中位数，OVER后可带分析函数子句NDV类似count(distinct )，但给出的是估算值，计算速度快NTILE将分区分到x个组上，每个给一个编号，配合PERCENTILE等使用PERCENT_RANK类似CUME_DIST，计算一组数的百分位分布，有聚合和分析两种用法，详见官方手册PERCENTILE_COUNT /&nbsp;PERCENTILE_DIST接受一个分位值，返回满足该分位值的插值后数值/集合中原始值，详见官方手册PERCENTILE只接受整型，计算p百分位数的值PERCENTILEAPPROX接受double型，计算p百分位数的值RANKRANK(expr [, expr ]…) WITHIN GROUP&nbsp; &nbsp;(ORDER BY&nbsp; &nbsp; expr [ DESC | ASC ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[ NULLS { FIRST | LAST } ]&nbsp; &nbsp; [, expr [ DESC | ASC ]&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [ NULLS { FIRST | LAST } ]&nbsp; &nbsp; ]…&nbsp; &nbsp;)计算排位值，有聚合和分析两种用法，这里的语法是聚合，重要，详见官方手册REGR一堆线性回归函数，不重要STAT_一堆统计函数，不重要STDDEV /&nbsp;STDDEV_POP /&nbsp;STDDEV_SAMP计算样本标准差、总体标准差、累积样本标准差，OVER后可带分析函数子句没有STDEV函数，没有OVER子句没有OVER子句SUMSUM([ DISTINCT | ALL ] expr)&nbsp; &nbsp;[ OVER (analytic_clause) ]OVER后可带分析函数子句同oracle同oracleSYS_XMLAGG /&nbsp;XMLAGG将一列的值组合为一个xml，其中SYS_XMLAGG课指定xml格式，XMLAGG可指定值排序方式VAR_POP / VAR_SAMP / VARIANCE计算样本方差、总体方差、累积样本方差，OVER后可带分析函数子句没有OVER子句，且VAR_POP和VARIANCE功能一样同oracle其中VAR_POP / VAR_SAMP也可写为VARIANCE_POP /VARIANCE_SAMP5.7 分析（开窗）函数分析（开窗）函数中，有一部分是和聚合函数同名的，只要可以带OVER子句的都可作为分析（开窗）函数使用，这部分不再重复列举。此外在列举函数前，需要对比一下三者的OVER子句和window子句的不同写法（主要差别就在于window子句）：OVER子句：Oracle：[ query_partition_clause ] [order_by_clause [ windowing_clause ] ]Hive：没找到细节定义，目测和Oracle一致Impala：和Oracle一致&nbsp;query_partition_clause:Oracle：PARTITION BY { expr[, expr ]…| (expr[, expr ]… ) }Hive：没找到细节定义，目测和Oracle一致Impala：没找到细节定义，目测和Oracle一致&nbsp;order_by_clause：Oracle：ORDER [ SIBLINGS ] BY { expr | position| c_alias } [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]&nbsp; [, { expr | position | c_alias } [ ASC | DESC] [ NULLS FIRST | NULLS LAST ] ]…Hive：没找到细节定义，目测和Oracle一致Impala：没找到细节定义，目测和Oracle一致&nbsp;windowing_clause：Oracle：{ ROWS | RANGE } { BETWEEN&nbsp;&nbsp; { UNBOUNDED PRECEDING&nbsp;&nbsp; | CURRENT ROW&nbsp;&nbsp; | value_expr { PRECEDING | FOLLOWING }&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;AND&nbsp;&nbsp; { UNBOUNDED FOLLOWING&nbsp;&nbsp; | CURRENT ROW&nbsp;&nbsp; | value_expr { PRECEDING | FOLLOWING }&nbsp;&nbsp; } | { UNBOUNDED PRECEDING&nbsp;&nbsp; | CURRENT ROW&nbsp;&nbsp; | value_expr PRECEDING&nbsp;&nbsp; } }Hive：ROWS ((CURRENT ROW) | (UNBOUNDED |[num]) PRECEDING) AND (UNBOUNDED | [num]) FOLLOWINGImpala：{ ROWS | RANGE } BETWEEN [ { m |UNBOUNDED } PRECEDING | CURRENT ROW] [ AND [CURRENT ROW | { UNBOUNDED | n }FOLLOWING] ]功能OracleHiveImpalaCUME_DIST有聚合和分析两种用法（上面已列）有分析用法DENSE_RANKDENSE_RANK( ) OVER([ query_partition_clause ] order_by_clause)这里的用法是分析同oracle同oracle（hive和impala只有分析用法没有聚合用法，故单列）FIRST_VALUE / LAST_VALUEFIRST_VALUE&nbsp;&nbsp; { (expr) [ {RESPECT | IGNORE} NULLS ]&nbsp; | (expr [ {RESPECT | IGNORE} NULLS ])&nbsp; }&nbsp; OVER (analytic_clause)返回某个排序集合的第一个/最后一个值FIRST_VALUE(expr) OVER([partition_by_clause] order_by_clause [window_clause])和oracle相比略简化同hiveLAGLAG&nbsp; { ( value_expr [, offset [, default]]) [ { RESPECT | IGNORE } NULLS ]&nbsp;&nbsp; | ( value_expr [ { RESPECT | IGNORE } NULLS ] [, offset [, default]] )&nbsp; }&nbsp; OVER ([ query_partition_clause ] order_by_clause)提供了一种同时访问表的多行的方式，即对访问的某行往前推offset行，避免了自连接，参考官方手册中取员工的本月和上月工资的例子LAG (expr [, offset] [, default])&nbsp; OVER ([partition_by_clause] order_by_clause)和oracle相比略简化同hiveLEADLEAD&nbsp; { ( value_expr [, offset [, default]] ) [ { RESPECT | IGNORE } NULLS ]&nbsp;&nbsp; | ( value_expr [ { RESPECT | IGNORE } NULLS ] [, offset [, default]] )&nbsp; }&nbsp; OVER ([ query_partition_clause ] order_by_clause)类似LAG，不同之处是往后推而不是往前推LEAD (expr [, offset] [, default])&nbsp; OVER ([partition_by_clause] order_by_clause)和oracle相比略简化同hivePERCENT_RANK有聚合和分析两种用法（上面已列）有分析用法RANKRANK( ) OVER ([ query_partition_clause ] order_by_clause)计算排位值，这里的用法是分析，有并列值时，下一位会跳开并列的个数再编号，如两个值并列第1，下一个值排第3。重要且常用同oracle同oracle（hive和impala只有分析用法没有聚合用法，故单列）ROW_NUMBERROW_NUMBER( )&nbsp; &nbsp;OVER ([ query_partition_clause ] order_by_clause)编行号，重要且常用同oracle同oracle5.8 其他&nbsp; &nbsp; &nbsp;剩下就是些各库特有的东西了，不常用，看看官方手册就好。&nbsp; &nbsp; &nbsp;整个总结下来有几点感悟。首先是Oracle实在太强大了，强大到以至于一旦用上瘾以后是没法解脱的，这就是为何有这么多企业宁可花大价钱堆一体机、RAC也不愿意改用开源数据库。其次是Oracle和Hive/Impala的语法还是有着巨大的差别，这种差别可能比大家想象得都要大，所以我之前考虑的将Oracle上的开发、分析师转为Hive侧的可能比预计的难。最后吐槽下Impala，实际应用中Impala用来做BI还是可以的，最大的槽点是不支持索引，这个真是让我百撕不得骑姐，很多场景下就是差了索引导致性能没法进一步提升，之前也看了点Impala的源码，猜想借用Hive元数据库的索引表或者在内存中单独管理都不是非常困难，谁能告诉我为什么……]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive 分区表增加column导入数据为空]]></title>
    <url>%2F2017%2F12%2F19%2FHive%20%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A2%9E%E5%8A%A0column%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E4%B8%BA%E7%A9%BA%2F</url>
    <content type="text"><![CDATA[hive1.1.0和hive1.2.1上，直接使用 alter table add column 分区表新增字段后新增字段值为空的情况。解决如下: 1234ALTER TABLE table ADD COLUMNS (duration bigint COMMENT '时长');ALTER TABLE table partition(day=20171219) ADD COLUMNS (duration bigint COMMENT '时长');]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java POI解析 Excel]]></title>
    <url>%2F2017%2F12%2F16%2FJava%20POI%20%E8%A7%A3%E6%9E%90Excel%2F</url>
    <content type="text"><![CDATA[坑1POI解析excel的时候, 取每个cell的值的时候需要先使用 cell.getCellType(), 去判断该cell的数据的类型(比如是 string还是boolean还是空之类的),这时候如果是日期类型,就会遇到”坑1”,日期类型是会被判断成_Cell.CELL_TYPENUMERIC ,如果想读取数据为指定时间格式,可以使用以下代码: 1234567SimpleDateFormat sdf = new SimpleDateFormat("yyy-MM-dd"); //日期格式化switch (cell.getCellType()) &#123; .... .... case Cell.CELL_TYPE_NUMERIC: if (DateUtil.isCellDateFormatted(cell)) &#123; value = sdf.format(cell.getDateCellValue()); 上面这个格式化数据代码还同时填了”坑1”自带的小坑, 算是个隐藏的坑, 那就是Excel中默认日期格式是没有”yyyy-mm-dd”这个格式的, 也就是如果Excel中的数据需要设置”yyyy-mm-dd”这样格式是需要自定义格式的, 如果你自定义了这种日期格式, 那么在POI读取的时候就会解析成奇怪的数据格式, 形如”16日-12月-2017年”等. 上面这个代码段也解决的这个小坑. 无论本来的Excel中数据格式是”yyyy/mm/dd” 还是”yyyy-mm-dd” 等都可以完美的解析成功. 坑2POI解析 0.00 和 0 这两种自然数数据, 我们会要求吧小数解析成”0.00”格式 而 整数解析成 0 不包含小数点的格式. 由于 小数和整数 都是cell.getCellType()==Cell.CELL_TYPE_NUMERIC, 所以需要一个方法一次性处理成功两种数据类型, 改要小数点后面的就保留, 不该要的(整数) 就不要加上烦人的整数.00这种. 代码如下: 1234567891011121314SimpleDateFormat sdf = new SimpleDateFormat("yyy-MM-dd"); //日期格式化switch (cell.getCellType()) &#123; .... .... case Cell.CELL_TYPE_NUMERIC: if (DateUtil.isCellDateFormatted(cell)) &#123; value = sdf.format(cell.getDateCellValue()); &#125; else &#123; Double d = cell.getNumericCellValue(); DecimalFormat decimalFormat = new DecimalFormat("#.##"); String numValue = decimalFormat.format(d); value = numValue; &#125; break; ps: 这种情况下有时候如果Entity使用Float的话会把小数解析成很长的一串, 比如本来数据是3.26, 他会解析成3.26000000323之类的,虽然换算成两位小数精度不变, 但是还是会影响存取显示. 所以你需要使用BigDecimal 而不是Float. 坑3这个坑填了好久, 主要是太奇葩了. 描述一下坑怎么来的:比如上传的Excel有10行数据,当你解析Excel的时候全部解析成功, 但是由于某种原因你不要第十行了, 那你直接选定第十行然后Del了 ,然后你再解析, 会发现还会读取第十行, 虽然你已在逐行读取的时候过滤设置了” row==null” 这个条件, 但就是会读取该行. 慢慢调节发现虽然删除了数据, 但是数据格式还保留在第10行, 逐行读取的时候还是会读取该行的数据格式作为一行,虽然没有数据. 除非你直接删除该行, 而不是简单使用常用的del. 由于用户还是会使用del 的,所以就要判断del数据但是还保存着数据格式的行. 代码如下: 123456789for (int j = 1; j &lt;= sheet.getLastRowNum(); j++) &#123; row = sheet.getRow(j); if (row == null) &#123; continue; &#125; if (new ImportExcelUtil().getCellValue(row.getCell(0)).toString().equals("") &amp; new ImportExcelUtil().getCellValue(row.getCell(1)).toString().equals("")) &#123; continue; &#125;]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>POI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[demo code]]></title>
    <url>%2F2017%2F12%2F12%2Fdemo%20code%2F</url>
    <content type="text"><![CDATA[mysql 数据更新时间字段1`update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',]]></content>
      <categories>
        <category>通用技术</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven dubbo-2.8.4导入教程]]></title>
    <url>%2F2017%2F12%2F07%2Fmaven%20dubbo-2.8.4%E5%AF%BC%E5%85%A5%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[maven中心库没有dubbo2.8.4,需要自己编译。我已经编译好了jar包 安装dubbo-2.8.4.jar包到maven仓库中如下操作： 1mvn install:install-file -Dfile=/Users/username/Downloads/dubbo-2.8.4.jar -DgroupId=com.alibaba -DartifactId=dubbo -Dversion=2.8.4 -Dpackaging=jar -DgeneratePom=true 需要dubbo-2.8.4.jar的联系我的邮箱就可以, 相信你会找到我的邮箱的]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala 上传本地数据]]></title>
    <url>%2F2017%2F11%2F30%2FImpala%20%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1. 使用location 方式这种方式下在impala的HDFS的db目录下直接指定一个和表名一样的目录就可以, 注意此目录当前在db目录下是不存在的. 也就是创建表的时候才会创建相应的目录. 说的不太清晰,请看代码就明白了 12345create external table device(group_id string ,device_id string)row format delimited fields terminated by '\t'lines terminated by '\n'location "hdfs://impala/warehouse/portal.db/device/" 注意这里建表是使用的外部表, external, EXTERNAL_TABLE 这样建的表被drop之后HDFS上面的文件依然还是存在的, 否则就是内部表, 删除表的同时HDFS上的文件一起删除了 注意这个/device/本来是不存在的, 创建表的时候才会创建这个目录后面需要加载到表的数据直接上传到这个目录下面就行, 然后再refresh +table name 一下,更新一下数据, 代码如下 123hadoop fs -put id.txt hdfs://impala/warehouse/portal.db/devicerefresh device # 在 impala shell 中进行 2.使用load方式这种方式是可以导入放在任意HDFS路径下的文件, 但是当文件被load的时候是采用的move方式, 也就是剪贴. 1234567# 建表create external table device1(group_id string ,device_id string)row format delimited fields terminated by '\t'lines terminated by '\n'# 加载数据load data inpath "hdfs://impala/tmp/id.txt" into (overwrite) table device 增加分区也可以使用这种方式, 假设我们按照group_id进行分区, 123456789101112# 建表create table device(device_id string)partitioned by (group_id string)row format delimited fields terminated by '\t'lines terminated by '\n'# 加载数据到新的分区# 增加分区数据(这里使用alter 是因为不存在当前分区,是新加的分区)alter table device add partition(dgroup_id="1") location "hdfs://...../id.txt"# 继续load数据到当前分区load data inpath "hdfs://hz-impala1/tmp/id.txt" into table device partition(group_id=1)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase 上传CSV文件]]></title>
    <url>%2F2017%2F11%2F29%2F%E6%8A%8Acsv%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%88%B0hdfs%20%2F</url>
    <content type="text"><![CDATA[把csv文件上传到hdfs12hadoop fs -mkdir /datahadoop fs -put file.csv /data hbase shell 新建表1create 'table','col1','col2' ####使用importtsv命令上传 1./hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,col1，col2 '-Dimporttsv.separator=,' table /data/file.csv 以上命令要把csv文件的第一列作为row，剩下的作为列族。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 下提示/boot空间不足解决办法]]></title>
    <url>%2F2017%2F11%2F29%2FUbuntu%20%3Aboot%20%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%2F</url>
    <content type="text"><![CDATA[查看当前内核1uname -a 查看内核12dpkg --get-selections|grep linuxlinux-后面带image的是旧的内核 清除不使用的内核123sudo apt-get autoremove linux-image-(版本号)或者sudo apt-get remove linux-image-(版本号)（就是上面带image的版本） 清理残留数据1dpkg -l |grep ^rc|awk '&#123;print $2&#125;' |sudo xargs dpkg -P]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive UDF 开发]]></title>
    <url>%2F2017%2F11%2F29%2FHive-UDF-%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[何为UDF?这个大家肯定已经都知道了,就是user defined function,当Hive内置函数不能满足你的业务需求的时候,就可以自己写一个UDF进行处理 需求需要把Hive表中含map格式的数据导入到impala中,在impala中使用map格式数据. 但是由于impala中取map格式数据的方式比较奇葩,无法进行map中的多个key进行group by. 同时为了方便拉取数据,所以暂时需要把hive中map转成json string 拉到impala中,然后再对json string进行解析. UDF编写过程1.自定义一个Java类 2.继承UDF类 3.重写evaluate方法 4.打成jar包 5.在hive执行add jar方法 6.在hive执行创建模板函数 7.hql中使用 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.datacube.hive.udf;import java.util.ArrayList;import java.util.Collections;import java.util.HashMap;import java.util.Map;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDF;@Description(name = "to_string", value = "_FUNC_(map) - Change map to String ")public class MapToString extends UDF&#123; public String evaluate(Map&lt;String, String&gt; a) &#123; if (a == null) &#123; return null; &#125; try &#123; ArrayList&lt;String&gt; r = new ArrayList&lt;String&gt;(a.size()); for (Map.Entry&lt;String, String&gt; entry : a.entrySet()) &#123; r.add("\""+entry.getKey()+"\"" + ":" + "\""+entry.getValue()+"\"" + ","); &#125; //Collections.sort(r); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; r.size(); i++) &#123; sb.append(r.get(i)); &#125; sb.deleteCharAt(sb.length() - 1); sb.insert(0,"&#123;"); sb.insert(sb.length(),"&#125;"); return sb.toString(); &#125;catch (Exception e)&#123; return null; &#125; &#125; public static void main(String[] args) &#123; Map&lt;String,String&gt; map =new HashMap&lt;String, String&gt;(); map.put("name","xiaoming"); map.put("age","18"); System.out.println(new MapToString().evaluate(map)); &#125; &#125; 下面是json string 的解析 写了两种方式,但是效率都不太高 123456789101112131415161718192021222324252627282930313233343536package com.datacube.hive.udf;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;@Description(name = "map_filter", value = "_FUNC_(string) - Get the Value of the Key ")public class MapFilter extends UDF &#123; public String evaluate(Text s, String target) &#123; if (s == null || target == null) &#123; return null; &#125; String[] kvPairs = s.toString().replace("\"", "").replace("&#125;","").replace("&#123;","").split(","); try &#123; String targetValue = null; for (String kvPair : kvPairs) &#123; String[] kv = kvPair.split(":"); String key = kv[0]; String value = kv[1]; if (key.equals(target)) &#123; targetValue=value; &#125; &#125; return targetValue; &#125; catch (Exception e) &#123; return null; &#125; &#125; public static void main(String[] args) &#123; String aa = "&#123;\"rid\":\"1511321168403\",\"column\":\"头条\",\"RCC\":\"D3RFE69O0001875P\",\"type\":\"doc\",\"offset\":\"2\"&#125;"; System.out.println(new MapFilter().evaluate(new Text(aa),"column")); &#125;&#125; 另一种解析方式是用的Gson 1234567891011121314151617public class MapFilterUDF extends UDF&#123; public String evaluate(Text s, String targetKey) &#123; if (s == null || targetKey == null) &#123; return null; &#125; try &#123; Gson gson = new Gson(); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map = gson.fromJson(s.toString(), map.getClass()); return map.get(targetKey); &#125; catch (Exception e)&#123; return null; &#125; &#125; 使用mvn package 打包,在Hive shell 中使用以下命令 123add jar "/...jar"CREATE TEMPORARY FUNCTION to_string AS '包名.MapToString(类名)';CREATE TEMPORARY FUNCTION map_filter AS '包名.MapFilter(类名)'; 可以使用description function +自定义函数名 查看描述 在UDF里面使用@Description已经把描述加进去了,方便使用者进行查看函数的描述]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摇滚客 | 蠢货王尼玛其人]]></title>
    <url>%2F2017%2F11%2F27%2F%E6%91%87%E6%BB%9A%E5%AE%A2%20%7C%20%E8%A0%A2%E8%B4%A7%E7%8E%8B%E5%B0%BC%E7%8E%9B%E5%85%B6%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[原文 原创来自微信公众号摇滚客,见文章最后二维码 距离《暴走大事件》全网下架过去整整十天了。 10天，240个小时，恍若隔世。 一个叫王尼玛的大头死变态，摘下了头套，活像一个“蠢货”。 作为一个知名脱口秀主持人，作为一个一千六百万粉丝的微博大V，竟然想不开去干愤青的事。 一面势单力薄地挑战不可能战胜的巨兽，一面发了疯似的煽动年轻人和他一起反抗，你说他蠢不蠢？ 豫章书院说没有学生自杀那就没人自杀，人家这么正规一学校能骗你吗？ 人家家长为了孩子好，跑去声援学校，你非骂人家“帮凶”，不封你封谁呢？ 人人都说皇帝的新衣五光十色，好看得不得了，你非毁谤人家没穿衣服。幼稚！可笑！ 活该节目被下架整改，回归遥遥无期，你骂学校、骂家长就算了，还固执地叫人家孩子动脑子，学会反抗。 喋喋不休，活像居委会大妈，人家又不是你儿子，你干嘛这么关心，一副恨铁不成钢的样子。 这不，这档烂摊子没解决，他又不怕死地跳出来，就红黄蓝幼儿园的事发声。 你自己说说你是不是蠢货？ （一） 80、90年代，曾经是文艺作品喷薄而出的春天。 那个年代的电影、电视剧创作没有这么多条条框框，更加注重个性。 特别是拍给孩子看的儿童剧、动画片多颜色荒诞，配乐诡异、剧情邪典，有着严重的反乌托邦色彩。 《魔方大厦》、《镜花缘》、《红气球》、《十二生肖》、《眉间尺》、《黑猫警长》，甚至是《葫芦娃》… 随便挑出一部，都能让看过的观众，记起一两个童年阴影的片段，至今记忆犹新。 很少人知道，这些苦涩的镜头，包含着那代艺术创作前辈们的良苦用心。 早年看黑猫警长，觉得动画片惩恶扬善，看犯罪份子被击毙有一种儿时才有的快感。 但—— 随着年龄渐涨，再看这部片子就发现了异端。 黑猫警长的世界实际一个法律一点都不健全的社会啊！ 警察局要靠秘密监视、窃听才能控制整个森林的稳定，时时消灭异己，才能保证和谐。 他们是怎么面对伏法的罪犯的？ 白鸽警员将罪犯的眼睛遮住，五花大绑，用皮带打得皮开肉绽，一旁的警长却无动于衷，眼神冰冷。 相信很多人还记得螳螂夫妇那集，对杀死蝗虫群的镜头，不断地进行特写： 螳螂夫妇挥刀斩杀了大批飞来的蝗虫，有的被腰斩、有的被肢解，还有的直接断了头。 接着警长到达现场，一把火烧死了这些蝗虫，并且大笑着说： “哈哈哈，这是最好的肥料啦！” 满地尸体、残肢乱飞的场面，很容易造成小观众的不适。 至于导演为什么这么拍，有人说，极有可能是创作人员在宣泄对83年严打的恐惧： 那个时代，组织家庭舞会会因为邻居的举报坐牢，偷窃少量财物会被判死刑。 当年有个叫当红的歌手叫做迟志强，就因为跳舞的舞伴坐在腿上，坐了四年牢，理由是聚众淫乱。 和他一起参加舞会的年轻人，无一幸免，一个小伙因为偷看女厕，判了死刑，缓期两年。 很多现代的年轻人对次并不了解，只有那些触目惊醒的往事还在述说那段讳莫如深的历史： 四川一个小伙和朋友打赌，亲了路边一陌生女孩，结果直接被枪毙，一命呜呼； 某青年因为喝多了在路边小解，被抓了现行，二话不说扭送新疆劳改； 还有一个男青年，看见街上的洋妞长得丰满，一时冲动摸了一把，也被处以枪决； … 拍完第五集《黑猫警长》就被紧急叫停了，至于原因导演有这样一段描述： 那天我被叫去人事处，他们递给我一张退休证，说我年龄到了，该退了。醒过神来后，我一句话没说，拿了退休证转身就走。 现在你还觉得《黑猫警长》仅仅是一部惩恶扬善的动画片吗？ 如果是的话你可以自己观察黑猫警长的武器——鲁格枪，纳粹盖世太保的标配，集权、恐怖的代名词。 第五集请看下集几个大字之后，便再也没有了消息… （二） 其实，给更多人留下童年阴影的片子是《魔方大厦》。 如果让滚君用两个字来形容这个片子，只有两个字—— 癫狂。 死人脸、怪诞的场景、诡异的剧情，不管是导演还是原作，都在尽力用隐晦的语言向我们表达些什么。 第二集，来克被一群小将们选为市长，上台后他将家长被关进了罐头，并贴上大字报，无情批判。 孩子们从此无拘无束，尽情玩耍打砸抢烧、让民生社会陷入了一片黑暗之中，但越是这样，市民们就约支持来克。 也许，创作者就是希望用这种方式，告诉那代孩子，无论什么时候都不要丧失理智，千万不要重蹈历史的覆辙。 长相怪异、尸白吊眼的干部 无法无天打砸抢烧的小将 还有消失在历史深处的大字报 还有，在个国家，人民不允许出现负面情绪，必须活在正能量中。 那些脸上笑嘻嘻，心理mmp的人，都会被集中起来，用特殊设备抽干净你的情绪。 从此，你就变成一个不哭不闹的乖孩子了。 除了一个叫怪里怪气的人，他不服为什么别人说好，他就要说好，为什么别人笑，他就要笑—— 他是一个独立的人啊！ 于是他想尽各种方法和这个世界作对，只不过，这一切都是徒劳，他最终战胜了自己，他爱老大哥。 在魔方大厦里，作者笑话失去个性的人终生都无法摆脱“禁锢的头脑，可笑的规矩”，还直言“不能说真话不如当哑巴！” 他还对“卫生部门的门外汉领导，非要插手文艺作品的审核”，进行了无情嘲弄，嬉笑怒骂快意恩仇。 诸如此类的情节还很多，这也就是为什么这部片子在播了十集之后，就遭到腰斩。 剩下的十六集遥遥无期，来克也被永远地关在了那座魔方里，出不来了。 有的时候，我们心疼来克再不能回家，可是看看我们，岂不是跟他一样，也迷失在一个现实版的“魔方大厦”里了。 在魔方大厦里，作者笑话失去个性的人终生都无法摆脱“禁锢的头脑，可笑的规矩”，还直言“不能说真话不如当哑巴！” 他还对“卫生部门的门外汉领导，非要插手文艺作品的审核”，进行了无情嘲弄，嬉笑怒骂快意恩仇。 诸如此类的情节还很多，这也就是为什么这部片子在播了十集之后，就遭到腰斩。 剩下的十六集遥遥无期，来克也被永远地关在了那座魔方里，出不来了。 有的时候，我们心疼来克再不能回家，可是看看我们，岂不是跟他一样，也迷失在一个现实版的“魔方大厦”里了。 （三） 岂止是动画片，80、90后接触到的电视剧、电影，很多都被那个时代的艺术工作者赋予了隐喻。 比如那部号称情景喜剧的鼻祖《我爱我家》，第一集就用春秋笔法映射了反复无常的政治运动。 还有那部《疯狂的兔子》，将人群在失去判断力之后的癫狂，庸众的可怕表现得淋漓尽致。 唯有一直清醒，坚定自我的人，才能走到最后… 公众号X博士说：那个年代的所有大师，都在暗暗教你如何成为一个独立而不屈的人。 创作者想尽各种办法把将经历过的痛苦、留下的伤疤通过另一种方式，传递给下一代。 即使他们明明知道这样的表达会造成下一代的反感，给他们留下童年阴影。 但，他们心里清楚，这就像打防疫针，了解了社会的真相，才能更勇敢地走下去—— 动动脑子，你们得学会反抗，反抗啊，少年… （四） 生于1990年的王尼玛就是在这种环境下成长起来的。 不管他的教育经历是怎样的，曾经那些前辈们创作出来的东西，都在他的脑中起到了潜移默化的作用。 也许很多80、90后，比起00后，有一股子劲儿，大概就是这个原因吧。 我们的童年里没有低幼的喜羊羊、熊出没，而是在阴影下生存，在不屈中呐喊的技巧啊！ 后来王尼玛长大了，办了自己的脱口秀。 虽然他可能还没意识到，但他确实在重复着曾经那些前辈们做过的事。 他一直在用不同的观点鞭笞社会给年轻人的偏见和不公啊！ 他带上头条，想让这些尖锐的观点不那么敏感，但他还是刺痛了一些人。 他说，即使是跪着，前方的路，我们也会一起走完。 所以，在节目的最后他说了这样一件事： 一个读者想考研，但父母逼他考公务员，他现在很苦难不知道怎么办。 王尼玛没有开玩笑，而是忧心忡忡的说，语气里充满了恨铁不成钢的感觉： “你要考研，谁拦得住，自己打工赚钱，自己考研。养了五年的宠物，不想送人谁拦得住，自己租房自己养它，谁拦得住？” 因为充满热血的人，至死都是少年！ “动动脑子，动动脑子，少年，要反抗，反抗嘛。动动脑子嘛，动动脑子，反抗，动动脑子！” 这和二十年前那些前辈们所说的，有什么不同呢？ 只不过那些前辈，用更聪明、更隐晦的方式。而王尼玛，就这样傻傻地送了人头。 除了暴走大事件，几乎所有同类节目都面临着寒冬期。 凤凰卫视官方发布消息，节目走过了19个年头后迎来停播。 关于节目为何停播的消息众说纷纭，当在微博上搜索关键字时，只有明晃晃的一排字： “根据相关法律法规和政策，搜索结果未予显示。 但其实这几乎是一个必然。有网友发现，一切早有预兆。 根据镜像娱乐调查显示：“在《锵锵》之前，“老梁”系列、《鲁豫有约》、《超级访问》、《壹周立波秀》、《今晚80后脱口秀》、《金星秀》等都没能逃过停播的命运。” 语言类、谈话类节目一一阵亡，有的销声匿迹，有的则转战网络。 但是网络环境又真的有想象中的那么好吗？ 6月份，“新浪微博”、“ACFUN”、“凤凰网”的视听节目服务被关，一时之间地动山摇。 为了生存下来，“新浪微博”严禁上传长视频，A站濒临崩溃、B站要求实名。无数网络节目都改名的改名、下架的下架。 诚如网友所说：该来的总会来的，一切不和谐的综艺都会被毙掉，不分电视、网络，没有意外！ 终于，今天，王尼玛也闭上了嘴。 想到之前有人用这么两句话来形容李银河： 多么愚蠢，非要闹到不能张嘴，非要闹到自己出点事。这对许多中国人来说，难道不是愚蠢的吗？ 然而，吾辈永远要记得，他捍卫的不是他一个人的自由，而是千千万万你我他的自由。所做的就是在为了你们呐喊啊！ 有人说王尼玛愤青、有人说他自寻死路，但是他所做的，不都是在告诉年轻人，你们要思考，你们学会反抗啊！ 就算举身赴死，也要义不容辞！ 永远，永远也不要因为习惯了黑暗，就去为黑暗辩护。 永远要记得，为众人抱薪者，不可使其冻毙于风雪！ 暴走大事件没了，王尼玛还是坚持在微博上表明自己对红黄蓝事件的态度，他是蠢，还蠢的那么可爱。 套用一句老话来说： 尽管我们脑子里的反叛改变不了这个世界，但它却是我们真正拥有的东西，是我们最后一寸领土。 在那一寸领土里，我们是自由的！]]></content>
      <categories>
        <category>千里之堤,做只蚂蚁</category>
      </categories>
      <tags>
        <tag>和谐社会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 文件数量，大小统计]]></title>
    <url>%2F2017%2F11%2F27%2FHDFS-%E6%96%87%E4%BB%B6%E6%95%B0%E9%87%8F%EF%BC%8C%E5%A4%A7%E5%B0%8F%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[####统计HDFS文件数量大小，小于20M文件数量 hadoop fs -du -h / # 统计文件大小 hadoop fs -count / # 统计文件数量，返回的数据是目录个数，文件个数，文件总计大小，输入路径 hadoop fs -ls -R /path/data | grep ^- &gt; ~/data.txt #统计所有文件的信息，过滤文件夹,只统计文件.因为使用ls -l 之后,可以看到文件是”-“开头文件夹是”d”开头 再写个小python 123456import pandas as pdpath='/Desktop/data.txt'df=pd.read_table(path,delim_whitespace=True,names=[1,2,3,4,5,6,7,8]) # 统计数据一共8列print(len(df))df1=df[df[5]&lt;20971520] # 第五列是大小,取小于20M(换算成b)的文件数据print(len(df1)) # 统计数量]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala Complex Types]]></title>
    <url>%2F2017%2F11%2F26%2FImpala-Complex-Types%2F</url>
    <content type="text"><![CDATA[impala不支持直接insert complex type data,需要从hive中或者从parquet file中取出来.而且需要使用parquet格式的表.官方文档如下: Because the Impala INSERT statement does not currently support creating new data with complex type columns, or copying existing complex type values from one table to another, you primarily use Impala to query Parquet tables with complex types where the data was inserted through Hive, or create tables with complex types where you already have existing Parquet data files. If you have created a Hive table with the Parquet file format and containing complex types, use the same table for Impala queries with no changes. If you have such a Hive table in some other format, use a Hive CREATE TABLE AS SELECT … STORED AS PARQUET or INSERT … SELECT statement to produce an equivalent Parquet table that Impala can query. If you have existing Parquet data files containing complex types, located outside of any Impala or Hive table, such as data files created by Spark jobs, you can use an Impala CREATE TABLE … STORED AS PARQUET statement, followed by an Impala LOAD DATA statement to move the data files into the table. As an alternative, you can use an Impala CREATE EXTERNAL TABLE statement to create a table pointing to the HDFS directory that already contains the data files. 具体demo步骤如下: ####创建hive表 12345create table array_map_1(id string , column_name array&lt;string&gt;,info map&lt;string, string&gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'; ####导入数据到hive中 1LOAD DATA local INPATH '/home/weblog/demodata' INTO TABLE array_map_1 ; 1234567# 数据如下1|体育,娱乐,科技,社会|name:tom,age:16,city:beijing2|娱乐,社会,NBA,时尚,历史|name:jerry,age:173|汽车,NBA,健康|name:alice,city:shanghai4|视频,政务|name:harry,age:16,city:jinan5|体育,军事|name:danny,age:17,city:6|体育,政务,军事,话题,历史,社会|name:liming,city:dalian 因为hive中的parquet表map,array不支持直接导入txt之类的非parquet格式数据,我也暂时不知道怎么创建parquet格式文件,只能暂时创建普通表格导入数据,再select到一个parquet表中. ####创建新的parquet表格并导入数据 12create table array_map(id string,column_name array&lt;string&gt;,info map&lt;string, string&gt;) stored as parquet;insert into array_map select id,column_name,info from array_map_1; ####查看array_map的hdfs路径 12345describe formatted array_map;...| Location: | hdfs://localhost:20500/test-warehouse/tpch_nested_parquet.db/array_map ... ####查看hdfs具体路径 1234$ hdfs dfs -ls hdfs://localhost:20500/test-warehouse/tpch_nested_parquet.db/array_mapFound 4 items-rwxr-xr-x 3 dev supergroup 171298918 2015-09-22 23:30 hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/000000_0... ####在impala shell中把数据拉过去 123CREATE TABLE array_map LIKE PARQUET 'hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/000000_0' STORED AS PARQUET location 'hdfs://localhost:20500/blah/tpch_nested_parquet.db/array_map/'; 最终结果显示]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git notes]]></title>
    <url>%2F2017%2F11%2F24%2Fgit-notes%2F</url>
    <content type="text"><![CDATA[其他命令* git init 在当前路径下初始化创建一个repository * git config --global alias.st status 把status配置成st --global参数是全局参数， 也就是这些命令在这台电脑的所有Git仓库下都有用。配置文件在C:\Users\username\.gitconfig 工作区和暂存区- git diff 是工作区(work dict)和暂存区(stage)的比较 - git diff --cached 是暂存区(stage)和分支(master)的比较 ###撤销修改 * 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 * 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令 git reset HEAD file，就回到了场景1，第二步按场景1操作。 ###添加文件到Git仓库 - git add +文件名 注意，可反复多次使用，添加多个文件； - git commit -m +&quot;描述&quot;（每次修改，如果不add到暂存区，那就不会加入到commit中） - git status 命令可以让我们时刻掌握仓库当前的状态 - git diff +文件名 查看文件做出过哪些修改（需要在git add之前查看） ###回退 - git log 查看历史记录 git log --pretty=oneline 把每个记录显示成一行便于查看 - git reset --hard HEAD^ HEAD表示当前版本,HEAD^表示上一个，HEAD~100表示向上100个 （WIN10 下使用git reset --hard HEAD~） - git reset --hard 66e315f （commit id） - git reflog 查看历史输入记录，进而查到commit id 回退后想再撤回，只需要找到想撤回到的版本使用的commit命令生成的commit id就可以，比如 git reset –hard 66e315f 但是如果关了电脑之前的命令看不到的话，可以使用git reflog查看历史输入记录。Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL。 ###删除文件 - git rm + 文件名 删除版本库中的文件。 - git checkout -- +文件名 把版本中的文件恢复到工作区 ###添加远程库 - git remote add origin git@server-name:path/repo-name.git 要关联一个远程库， - git push -u origin master 关联后第一次推送master分支的所有内容； - git push origin master 此后，每次本地提交后，只要有必要，就可以使用命令 git push origin master推送最新修改； ###创建删除分支 - 查看分支： git branch - 创建分支 ： git branch +分支名 - 切换分支 ： git checkout +分支名 - 创建+切换分支： git checkout -b+分支名 在本地创建和远程分支 对应的分支， 使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； - 合并某分支到当前分支 ： git merge +分支名 - 删除分支： git branch -d +分支名 - 强行删除分支 git branch -D +分支名 - git merge --no-ff -m &quot;描述&quot; +分支名删除分支后，不会丢掉分支信息，仍然在远程库中保留。 合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 ###合并冲突Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。git log –graph –pretty=oneline –abbrev-commit 看到分支的合并情况。。。 ###Bug分支当前分支只完成一半不能提交，但是需要马上去另一个分支修复bug，需要使用 - git stash 当前工作现场“储藏”起来，等以后恢复现场后继续工作 - git stash list 查看储存的工作现场 - git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； - git stash pop，恢复的同时把stash内容也删了： ###多人协作步骤 - 首先，可以试图用git push origin branch-name推送自己的修改； - 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； - 如果合并有冲突，则解决冲突，并在本地提交； - 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream branch-name origin/branch-name。 ###标签 - git tag &lt;name&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id； - git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息； - git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;可以用PGP签名标签； - git tag可以查看所有标签。 - git push origin &lt;tagname&gt;可以推送一个本地标签； - git push origin --tags可以推送全部未推送过的本地标签； - git tag -d &lt;tagname&gt;可以删除一个本地标签； - git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。 ###忽略特殊文件创建 .gitignore文件，并push到github 忽略文件的原则是： 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 git add -f App.class如果文件被忽略可以强制添加 git check-ignore命令检查规则 例如： $ git check-ignore -v App.class .gitignore:3:*.class App.class ###远程主机 * git remote 查看远程库信息 git remote -v 查看更详细的信息 * git remote show &lt;主机名&gt; 可以查看该主机的详细信息 * git remote add &lt;主机名&gt; &lt;网址&gt; 用于添加远程主机 * git remote rm &lt;主机名&gt; 删除远程主机 * git remote rename &lt;原主机名&gt; &lt;新主机名&gt; 此笔记在学习廖雪峰Git教程进行的记录。见http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000]]></content>
      <categories>
        <category>通用技术</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我做梦穿上了红裙子]]></title>
    <url>%2F2012%2F08%2F16%2F%E6%88%91%E5%81%9A%E6%A2%A6%E7%A9%BF%E4%B8%8A%E4%BA%86%E7%BA%A2%E8%A3%99%E5%AD%90%2F</url>
    <content type="text"><![CDATA[那是在去年夏天，即将步入高三的我再次来到了这个既熟悉又显得陌生的城市。在这里，所有的灯红酒绿，车水马龙都是显得与我那么的格格不入。我惧怕于夜下这个城市的灯光，它只会湮没我依然不剩多少的归属感。 我自己在一个偏僻点的小区租了间房子，打算在这里为我的人生转折点奠基。我喜欢被孤独包围的自己，因为这样就不会面对这样那样的眼神，虽然我从来没有真正的面对过一次。 我每天把自己反锁在这所小房子里，被歪歪斜斜的书包围着，挥动着笔盲目地写着，算着，但总感觉怎么算都没有结果。我不喜欢外出，尤其是白天。就算是吃饭也总会到一个地方，或买回来，或匆匆吃完赶紧回来。我从来不知道自己在躲避着什么，但一直是这样的躲避着。我感觉只有在熟悉的地方才不会感到更多的陌生，也就意味着不会有这样那样异样的目光。我惧怕与被别人用眼神扫过，惧怕和别人的眼睛对视，哪怕是在熟悉不过的朋友，甚至是镜子中的自己。在内心深处，自己会认为在一个自己熟悉的地方才有资本不被别人嘲笑和嘲笑别人。 后来我才知道，原来被嘲笑和嘲笑的对象一直是自己。我对陌生有种莫名其妙的畏惧。买衣服我会一直去固定的店里，即使没有自己喜欢的，差不多也不会拒绝，理发店也是如此，即使会排上几个小时。因为我不知为什么会认为我熟悉这里而觉得了不起。也许，你们会认为我很自卑，当然，我会自信的承认这一切。 在很多时候我都不能认清自己，不断地假想着自己的形象，并在最后做着无聊的否定。这一切我都喜欢着，乐此不彼的继续着。我一直渴望着认识自己，清楚地明白自己。 我自己决定自己夜晚不要出去。但，那个夜晚以后，我改变了…… 那是燥热的一天，模模糊糊的空气，恍恍惚惚的日子。我无聊着这个日子，一个让人再也不想的日子。终于挨到了夜晚，我本来希望要先洗个凉水澡，然后去睡觉，来抛开这一天。屋里的风扇呼呼的转着，对着燥热没有什么作用。谁知屋漏偏逢连夜雨，停电了，在一个燥热的夏天的夜晚。只是一片漆黑，寂静。我思索着，是不是出去走走…… 我摸索着前行，好不容易走了出来。我只知道外面很黑。我不敢轻易迈出步子，犹犹豫豫，幽幽郁郁。我自己忽然笑了，莫名其妙的笑着。突然，我听到了相似的笑声，是个女声，同样的莫名其妙。我尽量适应着黑暗，并在黑夜里寻找着，发现，在自己前面，站着一个只看到红色的轮廓。脊背发凉，我意识到这是在郊区，漆黑的夜晚会遇到什么。是她的再一次笑声让我悬着的心落了下来，那是一种似曾相识的声音，打自己并不能想出什么。 我埋怨道：“你笑什么！吓我一跳！”“我笑你在笑”！她淡淡的回答着，并没有再说些什么。 向前走了几步，她马上就向后退。我笑着说：“你怕什么，我又不是坏人？怎么我不害怕你，你到怕起我来了！要知道，这样夜晚女人更让人害怕。更何况这是在郊区。”“我为什么怕你？”她用质疑的语气问着我。“蒽…蒽…我哪知道？”我不知所措的回答着她，怎么也没料到她会这样问我。她没有再说什么，我也陷入了沉默。我静静地站着，观察着她，因为实在是太黑了，只看到隐隐约约的影子：她穿着红红的裙子，不算高，但很瘦。也许是披着头发，也许是长辫子，或是短发。我怎么也看不清。只是有点红红的。更不用说她的脸。我不知道她大概多大，我也不想知道。 我默默的思索着，一个女孩为什么会在这么黑的夜晚出来，还竟然出现在荒僻的郊区。我的好奇心驱使着我开口，但习惯压制住了好奇。我很少和女孩搭讪，甚至是熟悉的女同学。我不习惯和女生说话，或者说是不敢，或其他什么原因，我至今也不从得知。还是她打破了这种僵局。 “你在这里干什么，这么黑，你不害怕吗？” “我哦–蒽–不怎么怕，除了刚才。”我有点紧张，一种道不出来的感觉。 “呵呵呵……”她又笑了起来。 我满脸通红，自己感谢这令我讨厌的黑夜，幸亏看她不到我的表情。热热的天，但此时并没有那么多的汗了，我也不知道为什么，也许是刚才吓的。 “额……”我断断续续的问着，“你叫什么？为什么这么黑的夜里出现在这里？以前没有见过你呀！” “两个问题，先回答那一个呢？”她反问着。 “随便!”因为我也不知道，甚至不知道刚才问了什么。 “随便往往是最难的。”她小声说着，又忽然加大声音，“我也不知道我叫什么，也不知道为什么会在这里，更不知道会碰到……” “碰到什么？”我急切的问着。 “不知道。”她再次淡淡地回答着。 “哦！”我也说不出什么，只能敷衍着。我在心里猜着她的来历，应该是附近小区里的吧。我感到十分的好奇，好奇着这个夜晚的一切。 “这么黑的夜晚你爸妈怎么会让你出来呢？夜晚，总是有些令人畏惧的。”我问着她，只想听到一个不再那么奇妙的答案。 “我习惯这一切，我喜欢夜晚。”她快乐地回答着。但我又不由自主的惊了一下，喜欢夜晚的女孩，在郊区。我摇摇头，不敢再继续想下去。 “你…你喜欢黑夜……”我不敢再说下去，战战兢兢的。 “我是个人类耶。”她大声告诉我。似乎知道了我在害怕着什么。 “不是，你别误会，我……我其实……”她并没有等我说完。“没什么，一个女孩在黑夜里出现在这里，对你来说，是挺让人害怕的。我喜欢在夜晚出来，因为漆黑会吞噬着一切，无论美好还是险恶。” “你的爱好挺特别的。但为什么呢？”我更加好奇了，“除了你，没有人再喜欢漆黑了吧！” “会有的，而且会有很多。你呢？” “我-——”我突然不知道怎么回答，竟会在这个问题上犹豫。我在犹豫什么，我为什么不回答“不”呢？我徘徊着，思索着。也许我也喜欢黑夜，黑夜可以抹掉一切陌生的眼神，陌生的身影。 她没有再追问我。只是默默地说着：“黑夜是一个让人感到亲切的时刻，人们看不到我，我也看不到他们。无论我是漂亮还是丑陋，不会有人赞叹或是厌恶，只是平平淡淡，一片模糊。你和我相对，只是对方的一个轮廓。不会有什么形象上的印象，或是看法。”我感到了什么，这也许是我为什么犹豫，也许不是。 “但夜晚总会让很多人厌恶，他们喜欢白昼下的细腻的形象。有时让人感到更亲切。”我再考虑着我所谓的他们是否包括着我。 “夜晚是一个平等的世界，因为在这一刻，只有声音属于自己。所有的生命只是模模糊糊。尤其是现在，停电的时刻，一切并没有什么区别。”她似乎并没有听到我的反驳，依然自述着黑夜的美好。我也不明白自己在为谁反对着，只知道不是为了自己。 “听你说起来晚上挺美好的。”我没有理由再反对她，站在这里，我确实感到了她所说的一切。我不知道她长什么样子，她也是。我们没有一点印象在对方眼里，除了所谓的对话。 “再见！”她突然冒出一句，还竟然是来不及挥手的告别。可能我打扰了她的夜晚，也打扰了自己的。 “我们能在白天见面吗？”我感受到了她的美好，确实想见见她，想知道一个喜欢黑夜的女孩会长什么样子，是特别漂亮，还是…… “也许吧。”她轻轻地回答。我从她的语气里感不到丝毫的期待和拒绝。她静静的走开了，不知道那个方向。我只记得一个红红的影子。 在外面静静地走着，不知什么时候来电了。我只知道在城中心的那个方向，很亮，很刺眼。来电了，并没有丝毫的欣喜。只知道黑夜里出现了渐渐清晰地影子。 我想，这时候红裙子的女孩也回家了吧。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
</search>
